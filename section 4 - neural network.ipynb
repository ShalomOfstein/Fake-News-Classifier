{
 "cells": [
  {
   "cell_type": "raw",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d7834e9ef929b411"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Load the Required Libraries\n",
    "In this step, we import the necessary libraries, including PyTorch for building the model,\n",
    "and other utilities for data preprocessing, loading, and splitting."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33bdfde27f1542c6"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import KeyedVectors "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:53:35.019679700Z",
     "start_time": "2025-01-12T13:53:31.093214500Z"
    }
   },
   "id": "50344716be4f0221"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e4b4598463541a6b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:53:35.030639900Z",
     "start_time": "2025-01-12T13:53:35.021245800Z"
    }
   },
   "id": "6208ab06a04806cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Load and Preprocess the Dataset\n",
    "Here, we load the dataset from the CSV files and preprocess it for training the model.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5deed5f18d2a6425"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:53:36.047056300Z",
     "start_time": "2025-01-12T13:53:35.028734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the preprocessed data from CSV files\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "val_data = pd.read_csv(\"val.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "X_train = train_data['text']\n",
    "y_train = train_data['label']\n",
    "\n",
    "X_val = val_data['text']\n",
    "y_val = val_data['label']\n",
    "\n",
    "X_test = test_data['text']\n",
    "y_test = test_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Load Pre-trained Word2Vec Embeddings\n",
    "We use pre-trained Word2Vec embeddings to represent words as dense vectors.\n",
    "These embeddings improve the performance of the model by leveraging semantic relationships between words."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88ec74b5c796343c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec model\n",
    "word2vec = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:54:17.530038700Z",
     "start_time": "2025-01-12T13:53:36.048056400Z"
    }
   },
   "id": "fec26f2549a31896"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Create a vocabulary\n",
    "embedding_dim = 300\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # Special tokens\n",
    "embedding_matrix = [np.zeros(embedding_dim), np.random.uniform(-0.01, 0.01, embedding_dim)]  # Initialize <PAD> and <UNK>"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:54:17.543285800Z",
     "start_time": "2025-01-12T13:54:17.533038400Z"
    }
   },
   "id": "a330f9e5d96c4b5a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 76131\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary from Word2Vec\n",
    "for text in X_train:\n",
    "    for word in text.split():\n",
    "        if word not in vocab and word in word2vec:\n",
    "            vocab[word] = len(vocab)\n",
    "            embedding_matrix.append(word2vec[word])\n",
    "\n",
    "embedding_matrix = np.array(embedding_matrix)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:54:21.071420700Z",
     "start_time": "2025-01-12T13:54:17.542278800Z"
    }
   },
   "id": "bd9faee07f3ddc27"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#$ Step 4: Tokenize and Pad Sequences\n",
    "Convert the text into sequences of integers based on the vocabulary. We also pad sequences to ensure they all have the same length for batch processing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df38d9eec78e73f5"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Tokenize and convert text to sequences\n",
    "def text_to_sequence(text, vocab, max_len=200):\n",
    "    sequence = [vocab.get(word, vocab[\"<UNK>\"]) for word in text.split()]\n",
    "    if len(sequence) < max_len:\n",
    "        sequence.extend([vocab[\"<PAD>\"]] * (max_len - len(sequence)))\n",
    "    return sequence[:max_len]\n",
    "\n",
    "# Apply tokenization\n",
    "max_len = 200\n",
    "X_train_seq = [text_to_sequence(text, vocab, max_len) for text in X_train]\n",
    "X_val_seq = [text_to_sequence(text, vocab, max_len) for text in X_val]\n",
    "X_test_seq = [text_to_sequence(text, vocab, max_len) for text in X_test]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:54:25.344593500Z",
     "start_time": "2025-01-12T13:54:21.072418800Z"
    }
   },
   "id": "30f6dee9a41649a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Create a Dataset and DataLoader\n",
    "We define a custom Dataset class to handle our data and create DataLoader objects\n",
    "to efficiently load data during training and validation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58bdcacd0bfa149a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:54:25.355188Z",
     "start_time": "2025-01-12T13:54:25.349171400Z"
    }
   },
   "id": "d808fcaa1d2b07c"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Create Dataset and DataLoader\n",
    "batch_size = 32\n",
    "train_dataset = TextDataset(X_train_seq, y_train)\n",
    "val_dataset = TextDataset(X_val_seq, y_val)\n",
    "test_dataset = TextDataset(X_test_seq, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:54:25.929209600Z",
     "start_time": "2025-01-12T13:54:25.353189700Z"
    }
   },
   "id": "bbd8254a9c8d28ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 6: Define the MLP Model\n",
    "The model consists of an embedding layer initialized with Word2Vec embeddings,\n",
    "followed by an layers, and a fully connected output layer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84d278bafdd53198"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dims=[512, 256, 128], output_dim=1):\n",
    "        super(MLPModel, self).__init__()\n",
    "        \n",
    "        # Embedding Layer with frozen weights\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=True,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        \n",
    "        # Calculate input dimension\n",
    "        input_dim = embedding_matrix.shape[1] * max_len\n",
    "        \n",
    "        # Create list to hold all layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        layers.append(nn.LayerNorm(hidden_dims[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(0.2))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            layers.append(nn.LayerNorm(hidden_dims[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], output_dim))\n",
    "        \n",
    "        # Combine all layers\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get embeddings and flatten\n",
    "        embedded = self.embedding(x)\n",
    "        flattened = embedded.view(embedded.size(0), -1)\n",
    "        \n",
    "        # Forward pass through all layers\n",
    "        return self.model(flattened)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:54:25.940531600Z",
     "start_time": "2025-01-12T13:54:25.930209Z"
    }
   },
   "id": "fe62ddba4cbd31a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7: Train the Model\n",
    "Train the model for multiple epochs and validate its performance on the validation set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fd337492427a405"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=15, learning_rate=1e-4):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    patience = 4\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (texts, labels) in enumerate(train_loader):\n",
    "            texts = texts.to(device)\n",
    "            # Ensure labels are float and proper shape\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass and ensure output shape matches labels\n",
    "            outputs = model(texts).squeeze(-1)  # Change is here\n",
    "            \n",
    "            # Ensure shapes match\n",
    "            if len(outputs.shape) == 0:\n",
    "                outputs = outputs.unsqueeze(0)\n",
    "            if len(labels.shape) == 0:\n",
    "                labels = labels.unsqueeze(0)\n",
    "                \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f'Epoch: {epoch}, Batch: {i}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        train_acc = correct / total\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_acc = evaluate_model(model, val_loader)\n",
    "        \n",
    "        print(f'Epoch: {epoch}')\n",
    "        print(f'Average Loss: {avg_loss:.4f}')\n",
    "        print(f'Training Accuracy: {train_acc:.4f}')\n",
    "        print(f'Validation Accuracy: {val_acc:.4f}')\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_mlp_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print('Early stopping triggered')\n",
    "                break\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_loader:\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            outputs = model(texts).squeeze(-1)\n",
    "            \n",
    "            if len(outputs.shape) == 0:\n",
    "                outputs = outputs.unsqueeze(0)\n",
    "            if len(labels.shape) == 0:\n",
    "                labels = labels.unsqueeze(0)\n",
    "                \n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return correct / total"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:54:25.975256600Z",
     "start_time": "2025-01-12T13:54:25.941476600Z"
    }
   },
   "id": "92f9c91de8288107"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shayg\\projects\\DL\\Fake-News-Classifier\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 0.7503\n",
      "Epoch: 0, Batch: 100, Loss: 0.3085\n",
      "Epoch: 0, Batch: 200, Loss: 0.1219\n",
      "Epoch: 0, Batch: 300, Loss: 0.1323\n",
      "Epoch: 0, Batch: 400, Loss: 0.1635\n",
      "Epoch: 0, Batch: 500, Loss: 0.1526\n",
      "Epoch: 0, Batch: 600, Loss: 0.0565\n",
      "Epoch: 0, Batch: 700, Loss: 0.0176\n",
      "Epoch: 0\n",
      "Average Loss: 0.1566\n",
      "Training Accuracy: 0.9429\n",
      "Validation Accuracy: 0.9648\n",
      "Epoch: 1, Batch: 0, Loss: 0.0140\n",
      "Epoch: 1, Batch: 100, Loss: 0.0100\n",
      "Epoch: 1, Batch: 200, Loss: 0.0152\n",
      "Epoch: 1, Batch: 300, Loss: 0.0065\n",
      "Epoch: 1, Batch: 400, Loss: 0.0247\n",
      "Epoch: 1, Batch: 500, Loss: 0.0055\n",
      "Epoch: 1, Batch: 600, Loss: 0.0098\n",
      "Epoch: 1, Batch: 700, Loss: 0.0048\n",
      "Epoch: 1\n",
      "Average Loss: 0.0370\n",
      "Training Accuracy: 0.9897\n",
      "Validation Accuracy: 0.9642\n",
      "Epoch: 2, Batch: 0, Loss: 0.0046\n",
      "Epoch: 2, Batch: 100, Loss: 0.0376\n",
      "Epoch: 2, Batch: 200, Loss: 0.0024\n",
      "Epoch: 2, Batch: 300, Loss: 0.0024\n",
      "Epoch: 2, Batch: 400, Loss: 0.0024\n",
      "Epoch: 2, Batch: 500, Loss: 0.0025\n",
      "Epoch: 2, Batch: 600, Loss: 0.0022\n",
      "Epoch: 2, Batch: 700, Loss: 0.0065\n",
      "Epoch: 2\n",
      "Average Loss: 0.0138\n",
      "Training Accuracy: 0.9967\n",
      "Validation Accuracy: 0.9639\n",
      "Epoch: 3, Batch: 0, Loss: 0.0015\n",
      "Epoch: 3, Batch: 100, Loss: 0.0017\n",
      "Epoch: 3, Batch: 200, Loss: 0.0012\n",
      "Epoch: 3, Batch: 300, Loss: 0.0015\n",
      "Epoch: 3, Batch: 400, Loss: 0.0013\n",
      "Epoch: 3, Batch: 500, Loss: 0.0009\n",
      "Epoch: 3, Batch: 600, Loss: 0.0011\n",
      "Epoch: 3, Batch: 700, Loss: 0.0270\n",
      "Epoch: 3\n",
      "Average Loss: 0.0077\n",
      "Training Accuracy: 0.9980\n",
      "Validation Accuracy: 0.9575\n",
      "Epoch: 4, Batch: 0, Loss: 0.0008\n",
      "Epoch: 4, Batch: 100, Loss: 0.0009\n",
      "Epoch: 4, Batch: 200, Loss: 0.0007\n",
      "Epoch: 4, Batch: 300, Loss: 0.0008\n",
      "Epoch: 4, Batch: 400, Loss: 0.0009\n",
      "Epoch: 4, Batch: 500, Loss: 0.0007\n",
      "Epoch: 4, Batch: 600, Loss: 0.0007\n",
      "Epoch: 4, Batch: 700, Loss: 0.0007\n",
      "Epoch: 4\n",
      "Average Loss: 0.0035\n",
      "Training Accuracy: 0.9994\n",
      "Validation Accuracy: 0.9618\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = MLPModel(\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    hidden_dims=[512, 256, 128],\n",
    "    output_dim=1\n",
    ").to(device)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Train model\n",
    "train_model(model, train_loader, val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T14:03:40.935670900Z",
     "start_time": "2025-01-12T13:54:25.970266Z"
    }
   },
   "id": "ca8f2a80031117b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 8: Test the Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db36dc4ba2a63461"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9671067352875363\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = model(texts).squeeze(1)  # Ensure outputs have the same shape as labels\n",
    "        preds = torch.round(torch.sigmoid(outputs)).cpu().numpy()\n",
    "        test_preds.extend(preds)\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    print(f\"Test Accuracy: {test_acc}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T14:03:48.406240700Z",
     "start_time": "2025-01-12T14:03:40.943673800Z"
    }
   },
   "id": "bdf381d59ff9f467"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def predict_text(model, text, vocab, max_len=200, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and convert text to sequence\n",
    "    sequence = [vocab.get(word, vocab[\"<UNK>\"]) for word in text.split()]\n",
    "    if len(sequence) < max_len:\n",
    "        sequence.extend([vocab[\"<PAD>\"]] * (max_len - len(sequence)))\n",
    "    sequence = sequence[:max_len]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    sequence_tensor = torch.tensor([sequence], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(sequence_tensor)\n",
    "        prediction = torch.sigmoid(output.squeeze())\n",
    "        probability = prediction.item()\n",
    "        binary_prediction = 1 if probability >= 0.5 else 0\n",
    "    \n",
    "    return binary_prediction, probability\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T14:03:48.420272500Z",
     "start_time": "2025-01-12T14:03:48.416254700Z"
    }
   },
   "id": "e7ac8118dd1747"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shayg\\AppData\\Local\\Temp\\ipykernel_15396\\1564722389.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_mlp_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = MLPModel(embedding_matrix=embedding_matrix).to(device)\n",
    "model.load_state_dict(torch.load('best_mlp_model.pth'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T14:03:48.785113100Z",
     "start_time": "2025-01-12T14:03:48.423279500Z"
    }
   },
   "id": "dd7be049ae8ed2a7"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Real\n",
      "Confidence: 0.99%\n",
      "Prediction: Fake\n",
      "Confidence: 99.22%\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "new_text1 = \"The president has proposed a bew change\"\n",
    "new_text2 = \"Donald trTrunp has proposed a new change\"\n",
    "prediction, probability = predict_text(model, new_text1, vocab)\n",
    "print(f\"Prediction: {'Fake' if prediction == 1 else 'Real'}\")\n",
    "print(f\"Confidence: {probability:.2%}\")\n",
    "\n",
    "prediction, probability = predict_text(model, new_text2, vocab)\n",
    "print(f\"Prediction: {'Fake' if prediction == 1 else 'Real'}\")\n",
    "print(f\"Confidence: {probability:.2%}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T14:06:07.587002300Z",
     "start_time": "2025-01-12T14:06:07.569591700Z"
    }
   },
   "id": "75510970f424643f"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for articles containing 'Trump':\n",
      "Total articles: 12425\n",
      "Fake news: 7068 (56.89%)\n",
      "Real news: 5357 (43.11%)\n",
      "Statistics for articles containing 'president':\n",
      "Total articles: 14852\n",
      "Fake news: 6934 (46.69%)\n",
      "Real news: 7918 (53.31%)\n"
     ]
    }
   ],
   "source": [
    "def analyze_training_bias(train_data, word_of_interest):\n",
    "    # Filter articles containing the word\n",
    "    contains_word = train_data[train_data['text'].str.contains(word_of_interest, case=False)]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_articles = len(contains_word)\n",
    "    fake_articles = contains_word[contains_word['label'] == 1].shape[0]\n",
    "    real_articles = contains_word[contains_word['label'] == 0].shape[0]\n",
    "    \n",
    "    print(f\"Statistics for articles containing '{word_of_interest}':\")\n",
    "    print(f\"Total articles: {total_articles}\")\n",
    "    print(f\"Fake news: {fake_articles} ({fake_articles/total_articles*100:.2f}%)\")\n",
    "    print(f\"Real news: {real_articles} ({real_articles/total_articles*100:.2f}%)\")\n",
    "\n",
    "# Usage\n",
    "analyze_training_bias(train_data, \"Trump\")\n",
    "analyze_training_bias(train_data, \"president\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T14:03:49.343058400Z",
     "start_time": "2025-01-12T14:03:48.806450900Z"
    }
   },
   "id": "4aeda3e04ba91866"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T14:03:49.343058400Z",
     "start_time": "2025-01-12T14:03:49.337354900Z"
    }
   },
   "id": "68041890dfbd6936"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
