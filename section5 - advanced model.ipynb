{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# This is the implementation for the advanced Model",
   "id": "251ae91e588221"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1: Load the Required Libraries\n",
    "In this step, we import the necessary libraries, including PyTorch for building the model,\n",
    "and other utilities for data preprocessing, loading, and splitting."
   ],
   "id": "c6ccff1c2ab39ab1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T11:58:24.760044Z",
     "start_time": "2025-01-10T11:58:24.743856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import KeyedVectors "
   ],
   "id": "20fb14cae315c8d0",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T11:58:24.862374Z",
     "start_time": "2025-01-10T11:58:24.855846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "3e469ebea2a471cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: Load and Preprocess the Dataset\n",
    "Here, we load the dataset from the CSV files and preprocess it for training the model.\n"
   ],
   "id": "4d8cd20801adc4f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T11:58:26.701087Z",
     "start_time": "2025-01-10T11:58:24.862374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the preprocessed data from CSV files\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "val_data = pd.read_csv(\"val.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ],
   "id": "b67962808134f8f",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T11:58:26.821985Z",
     "start_time": "2025-01-10T11:58:26.701087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = train_data['text']\n",
    "y_train = train_data['label']\n",
    "\n",
    "X_val = val_data['text']\n",
    "y_val = val_data['label']\n",
    "\n",
    "X_test = test_data['text']\n",
    "y_test = test_data['label']"
   ],
   "id": "4744d15671509378",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3: Load Pre-trained Word2Vec Embeddings\n",
    "We use pre-trained Word2Vec embeddings to represent words as dense vectors.\n",
    "These embeddings improve the performance of the model by leveraging semantic relationships between words."
   ],
   "id": "dc4bf228fb9e734b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T11:59:38.071976Z",
     "start_time": "2025-01-10T11:58:41.594513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load pre-trained Word2Vec model\n",
    "word2vec = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ],
   "id": "21c53656a6f9376c",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T11:59:52.461632Z",
     "start_time": "2025-01-10T11:59:52.413282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a vocabulary\n",
    "embedding_dim = 300\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # Special tokens\n",
    "embedding_matrix = [np.zeros(embedding_dim), np.random.uniform(-0.01, 0.01, embedding_dim)]  # Initialize <PAD> and <UNK>"
   ],
   "id": "7b23489db3e9e4cc",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T11:59:58.396719Z",
     "start_time": "2025-01-10T11:59:53.346021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build vocabulary from Word2Vec\n",
    "for text in X_train:\n",
    "    for word in text.split():\n",
    "        if word not in vocab and word in word2vec:\n",
    "            vocab[word] = len(vocab)\n",
    "            embedding_matrix.append(word2vec[word])\n",
    "\n",
    "embedding_matrix = np.array(embedding_matrix)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ],
   "id": "a51ac4fd2b340934",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 75781\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T12:00:24.293707Z",
     "start_time": "2025-01-10T12:00:24.272786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix[embedding_matrix.shape[0]-1])\n"
   ],
   "id": "c84e3ad10d76146e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75781, 300)\n",
      "[-0.0703125  -0.05419922  0.41601562  0.11669922  0.20117188 -0.22949219\n",
      "  0.07275391  0.08154297  0.48828125  0.3359375   0.30859375  0.11376953\n",
      "  0.31054688  0.17089844  0.16601562 -0.16699219  0.10058594 -0.27539062\n",
      " -0.09716797  0.265625    0.03320312  0.32226562 -0.33398438 -0.21777344\n",
      " -0.29882812  0.15234375  0.04956055  0.09765625  0.08496094 -0.33398438\n",
      "  0.23632812  0.20703125 -0.16992188  0.34960938  0.26171875 -0.390625\n",
      " -0.16601562 -0.19238281  0.01226807  0.04907227 -0.06982422 -0.35742188\n",
      " -0.25195312  0.203125   -0.00415039  0.26367188 -0.08544922 -0.04223633\n",
      " -0.39257812  0.22851562  0.04248047  0.39453125 -0.00372314 -0.18066406\n",
      " -0.06347656 -0.27539062 -0.15234375 -0.58984375  0.15039062  0.16894531\n",
      "  0.13183594  0.38671875 -0.53515625 -0.31445312  0.07568359 -0.390625\n",
      " -0.24804688  0.31640625 -0.28125     0.34375     0.05810547 -0.02197266\n",
      " -0.140625   -0.26757812  0.16992188 -0.12353516  0.16894531  0.16796875\n",
      " -0.33203125 -0.12451172  0.13085938  0.09863281 -0.13183594 -0.22070312\n",
      "  0.32617188 -0.02294922 -0.09326172  0.296875    0.12988281  0.40234375\n",
      "  0.05078125 -0.07910156 -0.03393555 -0.15234375 -0.18359375  0.01599121\n",
      " -0.01623535  0.07861328 -0.02355957 -0.05004883 -0.00331116  0.16308594\n",
      "  0.30078125  0.13671875  0.0135498  -0.30859375 -0.19238281  0.10058594\n",
      " -0.0612793  -0.3046875  -0.21191406 -0.02539062 -0.125       0.28710938\n",
      "  0.09521484 -0.06591797  0.08789062 -0.328125   -0.02258301 -0.18164062\n",
      "  0.11035156 -0.17285156 -0.13964844  0.05224609 -0.13964844 -0.10107422\n",
      "  0.16796875 -0.16503906 -0.10058594  0.09960938 -0.39257812 -0.03112793\n",
      "  0.1953125   0.03857422 -0.0008812   0.01855469 -0.09912109 -0.02441406\n",
      " -0.03222656  0.2109375  -0.07421875 -0.05639648  0.00723267 -0.17773438\n",
      "  0.29492188  0.0390625  -0.08251953 -0.17578125  0.125      -0.37890625\n",
      " -0.33007812 -0.00860596 -0.04711914  0.0213623   0.15820312 -0.0324707\n",
      "  0.28125    -0.15917969 -0.25976562 -0.48632812 -0.54296875 -0.24609375\n",
      "  0.04101562 -0.04345703  0.09375     0.36523438  0.0456543  -0.44921875\n",
      " -0.39453125 -0.04907227 -0.00325012 -0.17382812  0.09326172  0.33984375\n",
      " -0.17871094 -0.10498047  0.1875     -0.04394531 -0.4609375   0.06787109\n",
      " -0.4921875  -0.328125   -0.08349609 -0.28320312 -0.05200195  0.06640625\n",
      " -0.17285156  0.24316406 -0.09716797 -0.00564575 -0.13769531  0.11914062\n",
      "  0.00408936  0.17285156  0.10498047  0.11767578  0.03271484 -0.0390625\n",
      " -0.12255859 -0.68359375 -0.07177734  0.33984375  0.03369141  0.06738281\n",
      " -0.3671875   0.15917969  0.28125    -0.25195312  0.10595703  0.04760742\n",
      "  0.11572266  0.078125    0.41601562  0.27539062  0.22070312  0.33789062\n",
      "  0.00567627  0.09033203  0.10009766  0.3359375  -0.10986328  0.09423828\n",
      " -0.15917969  0.45507812 -0.31640625  0.3359375  -0.11865234  0.02111816\n",
      " -0.32421875 -0.37109375 -0.09326172  0.30078125  0.31835938  0.15917969\n",
      "  0.35351562  0.20117188 -0.03637695 -0.29101562  0.15332031 -0.12695312\n",
      "  0.22363281 -0.3984375  -0.12011719 -0.02893066  0.00799561 -0.18359375\n",
      " -0.04418945 -0.42578125 -0.15722656 -0.5703125  -0.27148438 -0.21972656\n",
      "  0.14453125 -0.08056641  0.4453125   0.01251221 -0.4921875   0.05712891\n",
      " -0.28320312 -0.33398438  0.125       0.10253906 -0.00765991  0.234375\n",
      "  0.77734375 -0.00300598 -0.47460938 -0.14453125 -0.06445312 -0.1953125\n",
      " -0.3203125  -0.20507812  0.55859375  0.27929688 -0.40234375 -0.52734375\n",
      " -0.25976562 -0.171875   -0.21972656  0.22265625  0.28515625  0.12109375\n",
      " -0.1875     -0.02539062 -0.25390625 -0.13378906 -0.27539062 -0.45898438\n",
      "  0.02160645  0.1875      0.04272461 -0.47265625 -0.27148438 -0.04882812\n",
      " -0.3828125   0.046875    0.02832031 -0.140625   -0.02001953  0.16113281]\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4: Tokenize and Pad Sequences\n",
    "Convert the text into sequences of integers based on the vocabulary.\n",
    "We also pad sequences to ensure they all have the same length for batch processing."
   ],
   "id": "b66b271b6556deb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T12:00:35.976239Z",
     "start_time": "2025-01-10T12:00:30.520322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize and convert text to sequences\n",
    "def text_to_sequence(text, vocab, max_len=200):\n",
    "    sequence = [vocab.get(word, vocab[\"<UNK>\"]) for word in text.split()]\n",
    "    if len(sequence) < max_len:\n",
    "        sequence.extend([vocab[\"<PAD>\"]] * (max_len - len(sequence)))\n",
    "    return sequence[:max_len]\n",
    "\n",
    "# Apply tokenization\n",
    "max_len = 200\n",
    "X_train_seq = [text_to_sequence(text, vocab, max_len) for text in X_train]\n",
    "X_val_seq = [text_to_sequence(text, vocab, max_len) for text in X_val]\n",
    "X_test_seq = [text_to_sequence(text, vocab, max_len) for text in X_test]"
   ],
   "id": "a0431b6d47823104",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 5: Create a Dataset and DataLoader\n",
    "We define a custom Dataset class to handle our data and create DataLoader objects\n",
    "to efficiently load data during training and validation."
   ],
   "id": "7fc365e7cb272e96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T12:00:40.959782Z",
     "start_time": "2025-01-10T12:00:40.950947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom Dataset Class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]"
   ],
   "id": "dbb99e959ca3e8c1",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T12:00:46.069342Z",
     "start_time": "2025-01-10T12:00:45.090862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create Dataset and DataLoader\n",
    "batch_size = 32\n",
    "train_dataset = TextDataset(X_train_seq, y_train)\n",
    "val_dataset = TextDataset(X_val_seq, y_val)\n",
    "test_dataset = TextDataset(X_test_seq, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ],
   "id": "b6862622acf93277",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 6: Define the LSTM Model\n",
    "The model consists of an embedding layer initialized with Word2Vec embeddings,\n",
    "followed by an LSTM layer, and a fully connected output layer."
   ],
   "id": "ae7a87718e2eb512"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T12:00:48.466942Z",
     "start_time": "2025-01-10T12:00:48.452161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        x = self.fc(hidden[-1])\n",
    "        return self.sigmoid(x)"
   ],
   "id": "95713157f12cafab",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T12:00:50.021065Z",
     "start_time": "2025-01-10T12:00:49.924104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "hidden_dim = 128\n",
    "output_dim = 1  # Binary classification\n",
    "model = LSTMClassifier(embedding_matrix, hidden_dim, output_dim).to(device)"
   ],
   "id": "76450497ec2e2e4",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T12:00:51.967675Z",
     "start_time": "2025-01-10T12:00:51.927227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "dd0347760bb043ab",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 7: Train the Model\n",
    "Train the model for multiple epochs and validate its performance on the validation set."
   ],
   "id": "9f84501b3d5b9f7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T12:30:00.067013Z",
     "start_time": "2025-01-10T12:00:56.798967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training Loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for texts, labels in train_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(texts).squeeze(1)  # Ensure predictions have shape [batch_size]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions, val_labels = [], []\n",
    "        for texts, labels in val_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            predictions = model(texts).squeeze(1)  # Ensure predictions have shape [batch_size]\n",
    "            val_predictions.extend(predictions.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_predictions = np.round(val_predictions)\n",
    "    accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ],
   "id": "24b6633cb71febea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.4622\n",
      "Validation Accuracy: 0.5391\n",
      "Epoch 2/10, Loss: 0.2720\n",
      "Validation Accuracy: 0.9556\n",
      "Epoch 3/10, Loss: 0.0868\n",
      "Validation Accuracy: 0.9711\n",
      "Epoch 4/10, Loss: 0.0569\n",
      "Validation Accuracy: 0.9773\n",
      "Epoch 5/10, Loss: 0.0250\n",
      "Validation Accuracy: 0.9824\n",
      "Epoch 6/10, Loss: 0.0272\n",
      "Validation Accuracy: 0.9819\n",
      "Epoch 7/10, Loss: 0.0085\n",
      "Validation Accuracy: 0.9835\n",
      "Epoch 8/10, Loss: 0.0031\n",
      "Validation Accuracy: 0.9831\n",
      "Epoch 9/10, Loss: 0.0024\n",
      "Validation Accuracy: 0.9823\n",
      "Epoch 10/10, Loss: 0.0034\n",
      "Validation Accuracy: 0.9843\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 8: Test the Model\n",
    "Evaluate the model's performance on the unseen test set."
   ],
   "id": "65d57a35aba516da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T12:31:03.337527Z",
     "start_time": "2025-01-10T12:30:53.182975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions, test_labels = [], []\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        predictions = model(texts).squeeze()\n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_predictions = np.round(test_predictions)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ],
   "id": "e7c4d4e6278cbf86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9864\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T12:31:36.987215Z",
     "start_time": "2025-01-10T12:31:36.693640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"lstm_model.pth\")"
   ],
   "id": "f07572c3132bb79c",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5493de848045d405"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
