{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-16T20:28:30.768148600Z",
     "start_time": "2025-01-16T20:28:25.087410400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score,ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb5a4928fa2f6b02"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Load and Preprocess the Dataset\n",
    "Here, we load the dataset from the CSV files and preprocess it for training the model.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8a0e0eefd22aa74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the preprocessed data from CSV files\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "val_data = pd.read_csv(\"val.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9853fd1770508024"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = train_data['text']\n",
    "y_train = train_data['label']\n",
    "\n",
    "X_val = val_data['text']\n",
    "y_val = val_data['label']\n",
    "\n",
    "X_test = test_data['text']\n",
    "y_test = test_data['label']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39cd84151873b73c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Load Pre-trained Word2Vec Embeddings\n",
    "We use pre-trained Word2Vec embeddings to represent words as dense vectors.\n",
    "These embeddings improve the performance of the model by leveraging semantic relationships between words."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee298e6f604e7af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec model\n",
    "word2vec = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "306550f651f0af6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a vocabulary\n",
    "embedding_dim = 300\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # Special tokens\n",
    "embedding_matrix = [np.zeros(embedding_dim), np.random.uniform(-0.01, 0.01, embedding_dim)]  # Initialize <PAD> and <UNK>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d188ebf12e020a2c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build vocabulary from Word2Vec\n",
    "for text in X_train:\n",
    "    for word in word_tokenize(text.lower()):\n",
    "        if word not in vocab and word in word2vec:\n",
    "            vocab[word] = len(vocab)\n",
    "            embedding_matrix.append(word2vec[word])\n",
    "\n",
    "embedding_matrix = np.array(embedding_matrix)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df3b6ec5357941e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix[embedding_matrix.shape[0]-1])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8db51b117d4160b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Tokenize and Pad Sequences\n",
    "Convert the text into sequences of integers based on the vocabulary.\n",
    "We also pad sequences to ensure they all have the same length for batch processing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e7994a543f6487d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tokenize and convert text to sequences\n",
    "def text_to_sequence(text, vocab, max_len=1000):\n",
    "    sequence = [vocab.get(word, vocab[\"<UNK>\"]) for word in word_tokenize(text.lower())]\n",
    "    if len(sequence) < max_len:\n",
    "        sequence.extend([vocab[\"<PAD>\"]] * (max_len - len(sequence)))\n",
    "    return sequence[:max_len]\n",
    "\n",
    "# Apply tokenization\n",
    "max_len = 1000\n",
    "X_train_seq = [text_to_sequence(text, vocab, max_len) for text in X_train]\n",
    "X_val_seq = [text_to_sequence(text, vocab, max_len) for text in X_val]\n",
    "X_test_seq = [text_to_sequence(text, vocab, max_len) for text in X_test]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0207d893ad8c28d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f908ae29465025b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create Dataset and DataLoader\n",
    "batch_size = 32\n",
    "train_dataset = TextDataset(X_train_seq, y_train)\n",
    "val_dataset = TextDataset(X_val_seq, y_val)\n",
    "test_dataset = TextDataset(X_test_seq, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a52a5a22e4683a6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Step 6: Define the MLP Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4971b9a7899e0cc2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_matrix, hidden_dims=[512, 256, 128], output_dim=1):\n",
    "        super(MLPModel, self).__init__()\n",
    "\n",
    "        # Embedding Layer with frozen weights\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=True,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        # Calculate input dimension\n",
    "        input_dim = embedding_matrix.shape[1] * max_len\n",
    "\n",
    "        # Create list to hold all layers\n",
    "        layers = []\n",
    "\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        layers.append(nn.LayerNorm(hidden_dims[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(0.5))\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i + 1]))\n",
    "            layers.append(nn.LayerNorm(hidden_dims[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], output_dim))\n",
    "\n",
    "        # Combine all layers\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get embeddings and flatten\n",
    "        embedded = self.embedding(x)\n",
    "        flattened = embedded.view(embedded.size(0), -1)\n",
    "\n",
    "        # Forward pass through all layers\n",
    "        return self.model(flattened)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7f6932d00f9e7e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7: Initialize the Model \n",
    "We initialize the model with the embedding matrix."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa6163a74674cea1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = MLPModel(embedding_matrix).to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72603cc898d6b485"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7: Function for Evaluate the Model\n",
    "This method evaluates the model on the validation and test sets "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64859f659b74ccd3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_loader:\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            outputs = model(texts).squeeze()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predicted = torch.round(outputs)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(all_labels, all_predictions),\n",
    "        'precision': precision_score(all_labels, all_predictions),\n",
    "        'recall': recall_score(all_labels, all_predictions),\n",
    "        'f1': f1_score(all_labels, all_predictions)\n",
    "    }\n",
    "\n",
    "    return total_loss / len(data_loader), metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50f60dbac63b9190"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 8: Function for Train the Model\n",
    "We train the model using the training and validation datasets and monitor the loss and accuracy.\n",
    "To calculate the loss, we use the Binary Cross-Entropy loss function.\n",
    "We use the Adam optimizer to update the model parameters based on the gradients.\n",
    "To prevent overfitting, we make use of early stopping and learning rate scheduling.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19de3c2c7aa236ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=15, learning_rate=0.0005):\n",
    "    metrics_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'train_precision': [], 'val_precision': [],\n",
    "        'train_recall': [], 'val_recall': [],\n",
    "        'train_f1': [], 'val_f1': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    num_pos = sum(y_train == 1)\n",
    "    num_neg = sum(y_train == 0)\n",
    "    pos_weight = torch.tensor([num_neg / num_pos]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=0.1\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    best_model_state = None\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 4\n",
    "    patience_counter = 0\n",
    "    \n",
    "    num_warmup_steps = 100\n",
    "    def get_lr(step):\n",
    "        if step < num_warmup_steps:\n",
    "            return learning_rate * (step / num_warmup_steps)\n",
    "        return learning_rate\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_predictions = []\n",
    "        train_true_labels = []\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, (texts, labels) in enumerate(train_loader):\n",
    "            current_lr = get_lr(epoch * len(train_loader) + i)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "            \n",
    "            texts = texts.to(device)\n",
    "            # Ensure labels are float and proper shape\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass and ensure output shape matches labels\n",
    "            outputs = model(texts).squeeze(-1)\n",
    "            \n",
    "            # Ensure shapes match\n",
    "            if len(outputs.shape) == 0:\n",
    "                outputs = outputs.unsqueeze(0)\n",
    "            if len(labels.shape) == 0:\n",
    "                labels = labels.unsqueeze(0)\n",
    "                \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            train_predictions.extend(predicted.cpu().numpy())\n",
    "            train_true_labels.extend(labels.cpu().detach().numpy())\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f'Epoch: {epoch}, Batch: {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_acc = accuracy_score(train_true_labels, train_predictions)\n",
    "        train_precision = precision_score(train_true_labels, train_predictions)\n",
    "        train_recall = recall_score(train_true_labels, train_predictions)\n",
    "        train_f1 = f1_score(train_true_labels, train_predictions)\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_metrics = evaluate_model(model, val_loader, criterion)\n",
    "\n",
    "        # Store metrics\n",
    "        metrics_history['train_loss'].append(avg_train_loss)\n",
    "        metrics_history['val_loss'].append(val_loss)\n",
    "        metrics_history['train_acc'].append(train_acc)\n",
    "        metrics_history['val_acc'].append(val_metrics['accuracy'])\n",
    "        metrics_history['train_precision'].append(train_precision)\n",
    "        metrics_history['val_precision'].append(val_metrics['precision'])\n",
    "        metrics_history['train_recall'].append(train_recall)\n",
    "        metrics_history['val_recall'].append(val_metrics['recall'])\n",
    "        metrics_history['train_f1'].append(train_f1)\n",
    "        metrics_history['val_f1'].append(val_metrics['f1'])\n",
    "        metrics_history['learning_rates'].append(current_lr)\n",
    "\n",
    "        # Print epoch metrics\n",
    "        print(f\"\\nEpoch {epoch+1} Results:\")\n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Training Metrics: Acc={train_acc:.4f}, Prec={train_precision:.4f}, Rec={train_recall:.4f}, F1={train_f1:.4f}\")\n",
    "        print(f\"Validation Metrics: Acc={val_metrics['accuracy']:.4f}, Prec={val_metrics['precision']:.4f}, Rec={val_metrics['recall']:.4f}, F1={val_metrics['f1']:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            torch.save(best_model_state, 'best_mlp_model.pth')\n",
    "            print(\"â–º Saved new best model\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"\\nEarly stopping triggered!\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "        \n",
    "        scheduler.step(val_metrics['accuracy'])\n",
    "        \n",
    "\n",
    "    return metrics_history"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d979917318a56c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 9: Plot Training History\n",
    "We plot the training history to visualize the loss, accuracy, precision, recall, and F1 score over epochs.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2919837b1419df34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_training_history(metrics_history):\n",
    "    # Set up the style\n",
    "    # plt.style.use('seaborn')\n",
    "\n",
    "    # Create a figure with multiple subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Plot losses\n",
    "    ax1.plot(metrics_history['train_loss'], label='Training Loss')\n",
    "    ax1.plot(metrics_history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_title('Loss Over Time')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot accuracies\n",
    "    ax2.plot(metrics_history['train_acc'], label='Training Accuracy')\n",
    "    ax2.plot(metrics_history['val_acc'], label='Validation Accuracy')\n",
    "    ax2.set_title('Accuracy Over Time')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "\n",
    "    # Plot precision and recall\n",
    "    ax3.plot(metrics_history['train_precision'], label='Training Precision')\n",
    "    ax3.plot(metrics_history['val_precision'], label='Validation Precision')\n",
    "    ax3.plot(metrics_history['train_recall'], label='Training Recall')\n",
    "    ax3.plot(metrics_history['val_recall'], label='Validation Recall')\n",
    "    ax3.set_title('Precision and Recall Over Time')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Score')\n",
    "    ax3.legend()\n",
    "\n",
    "    # Plot F1 scores\n",
    "    ax4.plot(metrics_history['train_f1'], label='Training F1')\n",
    "    ax4.plot(metrics_history['val_f1'], label='Validation F1')\n",
    "    ax4.set_title('F1 Score Over Time')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('F1 Score')\n",
    "    ax4.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bf35c49e3090372"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 10: Plot the Confusion Matrix\n",
    "We plot the confusion matrix to visualize the model's performance on the test set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd0bfeaf37c699d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, test_loader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            texts = texts.to(device)\n",
    "            outputs = model(texts).squeeze(-1)\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_predictions))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8620f58685de571e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 11: Train the Model\n",
    "We train the model using the training and validation datasets and monitor the loss and accuracy.\n",
    "This process may take a while, depending on the number of epochs and the complexity of the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9ecb0ab40f3f9e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics_history = train_model(model, train_loader, val_loader, epochs=2, learning_rate=5e-4)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d78e513635815e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b3c8c0ab2395ddb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"MLP_model.pth\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c86db8a356ed2b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 12: Evaluate the Model\n",
    "plot the training history and evaluate the model on the test set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b221710425a1eec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_training_history(metrics_history)\n",
    "plot_confusion_matrix(model, test_loader)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7a846f4f301ef25"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### plot the loss over epochs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d151b6bd16013b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(metrics_history['train_loss'], label='Training Loss')\n",
    "plt.plot(metrics_history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs. Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.savefig(\"loss_vs_epochs_first try.png\")  # Save the current plot\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1f2a09ce3545f12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### plot the Accuracy over epochs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfc9f45d6d39fb26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(metrics_history['train_acc'], label='Training Accuracy')\n",
    "plt.plot(metrics_history['val_acc'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.savefig(\"accuracy_vs_epochs_first_try.png\")  # Save the current plot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6901bbbc2cb4a0c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### plot the precision, recall and f1-score over epochs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c77d9d56dd25064d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Precision\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(metrics_history['train_precision'], label='Training Precision')\n",
    "plt.plot(metrics_history['val_precision'], label='Validation Precision')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision vs. Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.savefig(\"precision_vs_epochs_first_try.png\")  # Save the current\n",
    "\n",
    "# Recall\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(metrics_history['train_recall'], label='Training Recall')\n",
    "plt.plot(metrics_history['val_recall'], label='Validation Recall')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall vs. Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.savefig(\"recall_vs_epochs_first_try.png\")  # Save the current plot\n",
    "\n",
    "# F1-Score\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(metrics_history['train_f1'], label='Training F1-Score')\n",
    "plt.plot(metrics_history['val_f1'], label='Validation F1-Score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('F1-Score vs. Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.savefig(\"f1_score_vs_epochs_first_try.png\")  # Save the current plot\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "753e4b4b34606a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 14: Print out the final evaluation metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e25908287d240236"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions, test_labels = [], []\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        predictions = model(texts).squeeze()\n",
    "        predictions = torch.round(torch.sigmoid(predictions))\n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "test_precision = precision_score(test_labels, test_predictions)\n",
    "test_recall = recall_score(test_labels, test_predictions)\n",
    "test_f1 = f1_score(test_labels, test_predictions)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "501af1fa3487b06e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
