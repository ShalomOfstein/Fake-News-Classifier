{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T17:16:10.349259Z",
     "start_time": "2025-01-16T17:16:06.285007200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from MLPModel import MLPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d80b677c4a52cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T17:16:10.375446600Z",
     "start_time": "2025-01-16T17:16:10.353261200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50014108420c9659",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T17:16:12.839677400Z",
     "start_time": "2025-01-16T17:16:10.372356400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the preprocessed data from CSV files\n",
    "new_data = pd.read_csv(\"../WELFake_Dataset.csv\")\n",
    "\n",
    "# split the new data into train and test\n",
    "new_train_data, new_test_data = train_test_split(new_data, test_size=0.2)\n",
    "\n",
    "X_train = new_train_data['text']\n",
    "X_train = pd.Series(X_train).fillna(\"\").tolist()\n",
    "y_train = new_train_data['label']\n",
    "\n",
    "X_test = new_test_data['text']\n",
    "X_test = pd.Series(X_test).fillna(\"\").tolist()\n",
    "y_test = new_test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e14867cd0f0ce5a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T17:16:52.877757800Z",
     "start_time": "2025-01-16T17:16:12.842174900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec model\n",
    "word2vec = KeyedVectors.load_word2vec_format(\"../GoogleNews-vectors-negative300.bin.gz\", binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e31b62e9e61cc96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T17:16:55.138833600Z",
     "start_time": "2025-01-16T17:16:52.918760800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 203269\n"
     ]
    }
   ],
   "source": [
    "# Load the training vocabulary and embedding matrix\n",
    "trained_state = torch.load('fine_tuned_model.pth', weights_only=True)\n",
    "original_embedding_weights = trained_state['embedding.weight']\n",
    "vocab_size = original_embedding_weights.shape[0]  # This should be 203269\n",
    "\n",
    "# Create vocabulary with same words as original model\n",
    "embedding_dim = 300\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # Special tokens\n",
    "embedding_matrix = [np.zeros(embedding_dim), np.random.uniform(-0.01, 0.01, embedding_dim)]\n",
    "\n",
    "# First add all words from original embedding matrix - THIS IS THE KEY CHANGE\n",
    "# We only add exactly the same number of words as in training\n",
    "for i in range(2, vocab_size):  # Skip PAD and UNK tokens\n",
    "    vocab[f\"word_{i}\"] = len(vocab)\n",
    "    embedding_matrix.append(original_embedding_weights[i].numpy())\n",
    "\n",
    "# Convert to numpy array\n",
    "embedding_matrix = np.array(embedding_matrix)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")  # Should print 203269\n",
    "\n",
    "# The rest of your tokenization and model loading code remains the same\n",
    "# Any words not in the vo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98af0d15546a97cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T17:16:55.150354500Z",
     "start_time": "2025-01-16T17:16:55.142833800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize and convert text to sequences\n",
    "def text_to_sequence(text, vocab, max_len=1000):\n",
    "    sequence = [vocab.get(word, vocab[\"<UNK>\"]) for word in text.split()]\n",
    "    if len(sequence) < max_len:\n",
    "        sequence.extend([vocab[\"<PAD>\"]] * (max_len - len(sequence)))\n",
    "    return sequence[:max_len]\n",
    "\n",
    "# Apply tokenization\n",
    "max_len = 1000\n",
    "X_test_seq = [text_to_sequence(text, vocab, max_len) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c91e1afdc299280f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-16T17:16:55.147833600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def augment_text(text):\n",
    "    words = text.split()\n",
    "    # Randomly drop some words (with 20% probability)\n",
    "    words = [w for w in words if random.random() > 0.2]\n",
    "    # If we dropped all words (unlikely but possible), return original text\n",
    "    if not words:\n",
    "        return text\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, is_training=False):\n",
    "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.float32)\n",
    "        self.is_training = is_training\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_training:\n",
    "            # Get the original sequence\n",
    "            sequence = self.texts[idx].tolist()  # Convert tensor to list\n",
    "            # Remove padding tokens for augmentation\n",
    "            sequence = [x for x in sequence if x != vocab[\"<PAD>\"]]\n",
    "            # Augment\n",
    "            if len(sequence) > 0:  # Only augment if we have tokens\n",
    "                sequence = [x for x in sequence if random.random() > 0.2]\n",
    "            # Re-pad the sequence\n",
    "            if len(sequence) < max_len:\n",
    "                sequence.extend([vocab[\"<PAD>\"]] * (max_len - len(sequence)))\n",
    "            sequence = sequence[:max_len]\n",
    "            # Convert back to tensor\n",
    "            text = torch.tensor(sequence, dtype=torch.long)\n",
    "        else:\n",
    "            text = self.texts[idx]\n",
    "            \n",
    "        return text, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3afac74fa4494c8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T17:16:55.154356800Z",
     "start_time": "2025-01-16T17:16:55.154356800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create Dataset and DataLoader\n",
    "batch_size = 32\n",
    "test_dataset = TextDataset(X_test_seq, y_test, is_training=False)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c93ea4d6260dfcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T17:16:55.168354800Z",
     "start_time": "2025-01-16T17:16:55.163358200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model with the same embedding matrix as training\n",
    "model = MLPModel(\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    hidden_dims=[256, 128, 64],\n",
    "    output_dim=1\n",
    ").to(device)\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(trained_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a18e383bd6c5bd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a7cea4927f290b6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-16T17:16:55.166357Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5118874332848132\n"
     ]
    }
   ],
   "source": [
    "X_test_seq = [text_to_sequence(text, vocab, max_len) for text in X_test]\n",
    "test_dataset = TextDataset(X_test_seq, y_test, is_training=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = model(texts).squeeze(1)  # Ensure outputs have the same shape as labels\n",
    "        preds = torch.round(torch.sigmoid(outputs)).cpu().numpy()\n",
    "        test_preds.extend(preds)\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4acbff0fcbe64",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-16T17:16:55.168354800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
