{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9510b63ade3fef3c",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "## This notebook is used to preprocess the data before training the model\n",
    "before running this notebook, make sure correct file paths are set\n",
    "- `fake.cvs`, `true.csv` and `WELFake_Dataset.csv` are in the 'data/original_data' folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7987ddc86023af08",
   "metadata": {},
   "source": "## Step 1: import the necessary libraries\n"
  },
  {
   "cell_type": "code",
   "id": "1cd8cdc7b8e135a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:30:20.641701Z",
     "start_time": "2025-01-18T21:30:20.624414Z"
    }
   },
   "source": [
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "bdc338b56ac9dcf4",
   "metadata": {},
   "source": "## Step 2: Load the datasets"
  },
  {
   "cell_type": "code",
   "id": "ec790828d6644460",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:30:23.259168Z",
     "start_time": "2025-01-18T21:30:20.641701Z"
    }
   },
   "source": [
    "fake_path = \"data/original_data/fake.csv\"\n",
    "true_path = \"data/original_data/true.csv\"\n",
    "WELF_dataset_path = \"data/original_data/WELFake_Dataset.csv\"\n",
    "\n",
    "fake_df = pd.read_csv(fake_path)\n",
    "true_df = pd.read_csv(true_path)\n",
    "WELF_dataset = pd.read_csv(WELF_dataset_path)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "7e647984c7d24244",
   "metadata": {},
   "source": "## Step 3: Add the label column to the datasets (Fake = 0, True = 1)"
  },
  {
   "cell_type": "code",
   "id": "6c4bb2850d5d9d95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:30:23.274844Z",
     "start_time": "2025-01-18T21:30:23.268342Z"
    }
   },
   "source": [
    "fake_df['label'] = 0\n",
    "true_df['label'] = 1"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Change the label of the WELF dataset to be in sync with our dataset",
   "id": "22ee580f35a258fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:30:23.292597Z",
     "start_time": "2025-01-18T21:30:23.286925Z"
    }
   },
   "cell_type": "code",
   "source": "WELF_dataset['label'] = 1 - WELF_dataset['label']",
   "id": "48ac7772f02564c7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 4: Data Exploration",
   "id": "3b033654c92e1f3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "check how many documents are in each dataset\n",
   "id": "49d58d1cc9438407"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:30:23.310527Z",
     "start_time": "2025-01-18T21:30:23.305371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"size of WELF dataset: \", len(WELF_dataset))\n",
    "print(\"size of fake dataset: \", len(fake_df))\n",
    "print(\"size of true dataset: \", len(true_df))"
   ],
   "id": "e2cb547d850a259e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of WELF dataset:  72134\n",
      "size of fake dataset:  23481\n",
      "size of true dataset:  21417\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "check how many fake and true documents are in the WELF dataset\n",
   "id": "9243b476b3b8cfc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:30:23.360483Z",
     "start_time": "2025-01-18T21:30:23.341100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"number of fake in WELF: \", len(WELF_dataset[WELF_dataset['label'] == 0]))\n",
    "print(\"number of true in WELF: \", len(WELF_dataset[WELF_dataset['label'] == 1]))"
   ],
   "id": "f1e6506350e9ae1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of fake in WELF:  37106\n",
      "number of true in WELF:  35028\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 4: Data Cleaning ",
   "id": "dc49880bdd65eb4a"
  },
  {
   "cell_type": "markdown",
   "id": "a054786a1500b2f",
   "metadata": {
    "collapsed": false
   },
   "source": " Remove documents that dont have a text from the WELF dataset"
  },
  {
   "cell_type": "code",
   "id": "25e169b32eed9cf5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-18T21:30:23.401893Z",
     "start_time": "2025-01-18T21:30:23.384118Z"
    }
   },
   "source": [
    "WELF_dataset = WELF_dataset.dropna(subset=['text'])"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "ad071e61e8c8254f",
   "metadata": {
    "collapsed": false
   },
   "source": " Leave only the title and text and label columns"
  },
  {
   "cell_type": "code",
   "id": "d7fbf08bd73bdf9d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-18T21:30:23.421799Z",
     "start_time": "2025-01-18T21:30:23.405902Z"
    }
   },
   "source": [
    "fake_df = fake_df[['title', 'text', 'label']]\n",
    "true_df = true_df[['title', 'text', 'label']]\n",
    "WELF_dataset = WELF_dataset[['title', 'text', 'label']]"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Removal of giveaway words\n",
    "In this section we will remove the words that are exclusive to one of the labels\n",
    "We realized that the word (Reuters) is exclusive to the true label and was affecting the results of the different models\n"
   ],
   "id": "2040b9d257c70338"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "check for each dataset how many times the frase (Reuters) appears in the text column for each label ",
   "id": "8c74fd9d499f6653"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:30:24.673543Z",
     "start_time": "2025-01-18T21:30:23.435535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"number of (Reuters) in true: \", true_df['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in fake: \", fake_df['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in WELF 'true' label: \", WELF_dataset[WELF_dataset['label'] == 1]['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in WELF 'fake' label: \", WELF_dataset[WELF_dataset['label'] == 0]['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())"
   ],
   "id": "9a3029581d974923",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of (Reuters) in true:  21246\n",
      "number of (Reuters) in fake:  0\n",
      "number of (Reuters) in WELF 'true' label:  21256\n",
      "number of (Reuters) in WELF 'fake' label:  0\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "remove (Reuters) from the text column in all the datasets",
   "id": "33fd630b25e3ef7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:30:28.266764Z",
     "start_time": "2025-01-18T21:30:24.683067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true_df['text'] = true_df['text'].str.replace(r'^.*?\\(Reuters\\)\\s*-\\s*', '', regex=True)\n",
    "fake_df['text'] = fake_df['text'].str.replace(r'^.*?\\(Reuters\\)\\s*-\\s*', '', regex=True)\n",
    "WELF_dataset['text'] = WELF_dataset['text'].str.replace(r'^.*?\\(Reuters\\)\\s*-\\s*', '', regex=True)\n",
    "\n",
    "#print out the number of (Reuters) in the text column for each label\n",
    "print(\"number of (Reuters) in true: \", true_df['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in fake: \", fake_df['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in WELF 'true' label: \", WELF_dataset[WELF_dataset['label'] == 1]['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in WELF 'fake' label: \", WELF_dataset[WELF_dataset['label'] == 0]['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())"
   ],
   "id": "6ce3753e7749b313",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of (Reuters) in true:  1\n",
      "number of (Reuters) in fake:  0\n",
      "number of (Reuters) in WELF 'true' label:  1\n",
      "number of (Reuters) in WELF 'fake' label:  0\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "conclusion: there is a document with the frase (Reuters) twice!\n",
    "print the entry where the remaining (Reuters) is in the text column"
   ],
   "id": "ce2a768efc9aad90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:30:29.806730Z",
     "start_time": "2025-01-18T21:30:28.288881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(true_df[true_df['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*')])\n",
    "print(WELF_dataset[WELF_dataset['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*')])"
   ],
   "id": "dc01b6964578a54b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  title  \\\n",
      "5882  The Trump presidency on Jan 29 at 4:12 P.M. ES...   \n",
      "\n",
      "                                                   text  label  \n",
      "5882  Jan 29 (Reuters) - Highlights of the day for U...      1  \n",
      "                                                   title  \\\n",
      "26543  The Trump presidency on Jan 29 at 4:12 P.M. ES...   \n",
      "\n",
      "                                                    text  label  \n",
      "26543  Jan 29 (Reuters) - Highlights of the day for U...      1  \n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "remove the final (Reuters) from the text column in all the datasets",
   "id": "2b4251055b87920e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:30:33.556006Z",
     "start_time": "2025-01-18T21:30:29.813737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true_df['text'] = true_df['text'].str.replace(r'^.*?\\(Reuters\\)\\s*-\\s*', '', regex=True)\n",
    "fake_df['text'] = fake_df['text'].str.replace(r'^.*?\\(Reuters\\)\\s*-\\s*', '', regex=True)\n",
    "WELF_dataset['text'] = WELF_dataset['text'].str.replace(r'^.*?\\(Reuters\\)\\s*-\\s*', '', regex=True)\n",
    "\n",
    "#print out the number of (Reuters) in the text column for each label\n",
    "print(\"number of (Reuters) in true: \", true_df['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in fake: \", fake_df['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in WELF 'true' label: \", WELF_dataset[WELF_dataset['label'] == 1]['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in WELF 'fake' label: \", WELF_dataset[WELF_dataset['label'] == 0]['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())"
   ],
   "id": "15b7480fb39d5892",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of (Reuters) in true:  0\n",
      "number of (Reuters) in fake:  0\n",
      "number of (Reuters) in WELF 'true' label:  0\n",
      "number of (Reuters) in WELF 'fake' label:  0\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Analyze Word Frequencies\n",
    "This section will analyze the word frequencies in the fake and true news articles.\n",
    "We will identify the most common words in each category and find words that are exclusive to one category.\n",
    "Once we have the exclusive words, we can decide whether to remove them or keep them in the dataset."
   ],
   "id": "74147a8b64eb2615"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:30:33.604218Z",
     "start_time": "2025-01-18T21:30:33.565538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_fake_word_check = pd.concat( [fake_df, WELF_dataset[WELF_dataset['label'] == 0]], axis=0).reset_index(drop=True)\n",
    "all_true_word_check = pd.concat( [true_df, WELF_dataset[WELF_dataset['label'] == 1]], axis=0).reset_index(drop=True)\n",
    "print(\"length of all_fake_word_check: \", len(all_fake_word_check))\n",
    "print(\"length of all_true_word_check: \", len(all_true_word_check))"
   ],
   "id": "8d6cbb67f258b4fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of all_fake_word_check:  60548\n",
      "length of all_true_word_check:  56445\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove duplicates from the datasets to avoid counting the same document multiple times",
   "id": "44d40bd9065cc6b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:30:34.518369Z",
     "start_time": "2025-01-18T21:30:33.615543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#remove duplicates from the datasets\n",
    "all_fake_word_check = all_fake_word_check.drop_duplicates()\n",
    "all_true_word_check = all_true_word_check.drop_duplicates()\n",
    "print(\"length of all_fake_word_check: \", len(all_fake_word_check))\n",
    "print(\"length of all_true_word_check: \", len(all_true_word_check))"
   ],
   "id": "8530c809fdfc90b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of all_fake_word_check:  28848\n",
      "length of all_true_word_check:  34791\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code block runs the analysis of the word frequencies in the fake and true news articles.\n",
    "It will print the total unique words in each vocabulary, the exclusive words in each category, and the top exclusive words.\n",
    "You can adjust the minimum frequency to filter out less common words."
   ],
   "id": "b884a1bd4c529e11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:30:57.596946Z",
     "start_time": "2025-01-18T21:30:34.529996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_text(text: str, keep_symbols: bool = True) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocess text while preserving multilingual characters and optional symbols.\n",
    "    \n",
    "    Parameters:\n",
    "        text: Input text\n",
    "        keep_symbols: If True, keeps emoji and special symbols\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    if keep_symbols:\n",
    "        # Keep unicode characters (including emojis) and letters, remove unwanted characters\n",
    "        # This pattern keeps all Unicode letters, numbers, emojis, and common symbols\n",
    "        text = re.sub(r'[^\\w\\s\\u0080-\\u10FFFF\\']+', ' ', text)\n",
    "    else:\n",
    "        # Remove special characters but keep apostrophes for contractions\n",
    "        text = re.sub(r'[^a-zA-Z\\']+', ' ', text)\n",
    "\n",
    "    # Remove single quotes if they're not part of contractions\n",
    "    text = re.sub(r'\\s\\'|\\'\\s|^\\'|\\'$', ' ', text)\n",
    "\n",
    "    # Split into words and remove empty strings\n",
    "    words = [word.strip() for word in text.split()]\n",
    "\n",
    "    # Remove single-character words and empty strings\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "\n",
    "    return words\n",
    "\n",
    "def analyze_word_frequencies(fake_df: pd.DataFrame, true_df: pd.DataFrame,\n",
    "                             text_column: str, min_frequency: int = 2,\n",
    "                             keep_symbols: bool = True) -> Tuple[Dict, Dict, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Analyze word frequencies in fake and true news articles.\n",
    "    Returns word frequencies and article counts for both categories.\n",
    "    \"\"\"\n",
    "    # Initialize Counters for word frequencies and article appearances\n",
    "    fake_vocab = Counter()\n",
    "    true_vocab = Counter()\n",
    "    fake_article_counts = Counter()\n",
    "    true_article_counts = Counter()\n",
    "\n",
    "    # Process fake news\n",
    "    for _, row in fake_df.iterrows():\n",
    "        text = row[text_column]\n",
    "        words = preprocess_text(text, keep_symbols)\n",
    "        # Update total word frequency\n",
    "        fake_vocab.update(words)\n",
    "        # Update article counts (count each word only once per article)\n",
    "        fake_article_counts.update(set(words))\n",
    "\n",
    "    # Process true news\n",
    "    for _, row in true_df.iterrows():\n",
    "        text = row[text_column]\n",
    "        words = preprocess_text(text, keep_symbols)\n",
    "        # Update total word frequency\n",
    "        true_vocab.update(words)\n",
    "        # Update article counts (count each word only once per article)\n",
    "        true_article_counts.update(set(words))\n",
    "\n",
    "    # Find exclusive words (filtering by minimum frequency)\n",
    "    fake_exclusive = {}\n",
    "    true_exclusive = {}\n",
    "\n",
    "    for word, count in fake_vocab.items():\n",
    "        if word not in true_vocab and count >= min_frequency:\n",
    "            fake_exclusive[word] = {\n",
    "                'total_frequency': count,\n",
    "                'article_count': fake_article_counts[word],\n",
    "                'article_percentage': (fake_article_counts[word] / len(fake_df)) * 100\n",
    "            }\n",
    "\n",
    "    for word, count in true_vocab.items():\n",
    "        if word not in fake_vocab and count >= min_frequency:\n",
    "            true_exclusive[word] = {\n",
    "                'total_frequency': count,\n",
    "                'article_count': true_article_counts[word],\n",
    "                'article_percentage': (true_article_counts[word] / len(true_df)) * 100\n",
    "            }\n",
    "\n",
    "    # Sort by total frequency\n",
    "    fake_exclusive_sorted = dict(sorted(fake_exclusive.items(),\n",
    "                                        key=lambda x: x[1]['total_frequency'],\n",
    "                                        reverse=True))\n",
    "    true_exclusive_sorted = dict(sorted(true_exclusive.items(),\n",
    "                                        key=lambda x: x[1]['total_frequency'],\n",
    "                                        reverse=True))\n",
    "\n",
    "    return fake_vocab, true_vocab, fake_exclusive_sorted, true_exclusive_sorted\n",
    "\n",
    "def print_analysis_results(fake_vocab: Dict, true_vocab: Dict,\n",
    "                           fake_exclusive: Dict, true_exclusive: Dict,\n",
    "                           n_words: int = 10):\n",
    "    \"\"\"\n",
    "    Print detailed analysis results including article counts.\n",
    "    \"\"\"\n",
    "    print(\"\\nVocabulary Statistics:\")\n",
    "    print(f\"Total unique words in fake news vocabulary: {len(fake_vocab):,}\")\n",
    "    print(f\"Total unique words in true news vocabulary: {len(true_vocab):,}\")\n",
    "    print(f\"Exclusive words in fake news: {len(fake_exclusive):,}\")\n",
    "    print(f\"Exclusive words in true news: {len(true_exclusive):,}\")\n",
    "\n",
    "    print(\"\\nTop exclusive words in fake news:\")\n",
    "    for word, stats in list(fake_exclusive.items())[:n_words]:\n",
    "        print(f\"'{word}': {stats['total_frequency']:,} total occurrences, \"\n",
    "              f\"appears in {stats['article_count']:,} articles \"\n",
    "              f\"({stats['article_percentage']:.1f}% of fake news articles)\")\n",
    "\n",
    "    print(\"\\nTop exclusive words in true news:\")\n",
    "    for word, stats in list(true_exclusive.items())[:n_words]:\n",
    "        print(f\"'{word}': {stats['total_frequency']:,} total occurrences, \"\n",
    "              f\"appears in {stats['article_count']:,} articles \"\n",
    "              f\"({stats['article_percentage']:.1f}% of true news articles)\")\n",
    "\n",
    "def analyze_specific_terms(fake_df: pd.DataFrame, true_df: pd.DataFrame,\n",
    "                           text_column: str, terms: List[str]):\n",
    "    \"\"\"\n",
    "    Analyze frequency of specific terms in both datasets.\n",
    "    \"\"\"\n",
    "    for term in terms:\n",
    "        # Count occurrences\n",
    "        true_count = true_df[text_column].str.count(term).sum()\n",
    "        fake_count = fake_df[text_column].str.count(term).sum()\n",
    "\n",
    "        # Count articles containing the term\n",
    "        true_articles = true_df[text_column].str.contains(term, case=False).sum()\n",
    "        fake_articles = fake_df[text_column].str.contains(term, case=False).sum()\n",
    "\n",
    "        true_percentage = (true_articles / len(true_df)) * 100\n",
    "        fake_percentage = (fake_articles / len(fake_df)) * 100\n",
    "\n",
    "        print(f\"\\nTerm '{term}':\")\n",
    "        print(f\"True news: {true_count:,} total occurrences, \"\n",
    "              f\"appears in {true_articles:,} articles \"\n",
    "              f\"({true_percentage:.2f}% of true news articles)\")\n",
    "        print(f\"Fake news: {fake_count:,} total occurrences, \"\n",
    "              f\"appears in {fake_articles:,} articles \"\n",
    "              f\"({fake_percentage:.2f}% of fake news articles)\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming fake_df and true_df are your separated DataFrames\n",
    "    fake_vocab, true_vocab, fake_exclusive, true_exclusive = analyze_word_frequencies(\n",
    "        all_fake_word_check, all_true_word_check,\n",
    "        text_column='text',\n",
    "        min_frequency=2,\n",
    "        keep_symbols=True  # Set to True to keep emojis and special characters\n",
    "    )\n",
    "\n",
    "    # Print general analysis\n",
    "    print_analysis_results(fake_vocab, true_vocab, fake_exclusive, true_exclusive)\n",
    "\n"
   ],
   "id": "75807659bf095d3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary Statistics:\n",
      "Total unique words in fake news vocabulary: 185,021\n",
      "Total unique words in true news vocabulary: 127,253\n",
      "Exclusive words in fake news: 38,083\n",
      "Exclusive words in true news: 24,928\n",
      "\n",
      "Top exclusive words in fake news:\n",
      "'что': 1,423 total occurrences, appears in 148 articles (0.5% of fake news articles)\n",
      "'не': 1,170 total occurrences, appears in 137 articles (0.5% of fake news articles)\n",
      "'21wire': 1,117 total occurrences, appears in 552 articles (1.9% of fake news articles)\n",
      "'quot': 1,012 total occurrences, appears in 13 articles (0.0% of fake news articles)\n",
      "'по': 766 total occurrences, appears in 149 articles (0.5% of fake news articles)\n",
      "'это': 727 total occurrences, appears in 101 articles (0.4% of fake news articles)\n",
      "'как': 603 total occurrences, appears in 133 articles (0.5% of fake news articles)\n",
      "'то': 482 total occurrences, appears in 105 articles (0.4% of fake news articles)\n",
      "'somodevilla': 481 total occurrences, appears in 480 articles (1.7% of fake news articles)\n",
      "'2017the': 463 total occurrences, appears in 395 articles (1.4% of fake news articles)\n",
      "\n",
      "Top exclusive words in true news:\n",
      "'rakhine': 908 total occurrences, appears in 273 articles (0.8% of true news articles)\n",
      "'mnangagwa': 395 total occurrences, appears in 92 articles (0.3% of true news articles)\n",
      "'tmsnrt': 334 total occurrences, appears in 265 articles (0.8% of true news articles)\n",
      "'barnier': 283 total occurrences, appears in 116 articles (0.3% of true news articles)\n",
      "'marawi': 273 total occurrences, appears in 71 articles (0.2% of true news articles)\n",
      "'babis': 267 total occurrences, appears in 41 articles (0.1% of true news articles)\n",
      "'durst': 242 total occurrences, appears in 11 articles (0.0% of true news articles)\n",
      "'pamkeynen': 238 total occurrences, appears in 238 articles (0.7% of true news articles)\n",
      "'kem': 219 total occurrences, appears in 59 articles (0.2% of true news articles)\n",
      "'sokha': 217 total occurrences, appears in 59 articles (0.2% of true news articles)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Analyze Specific Terms\n",
    "This section will analyze the frequency of specific terms in both datasets.\n",
    "You can include emojis and non-English terms in the list of terms to analyze.\n",
    "If there are specific terms you want to check, you can add them to the `terms_of_interest` list."
   ],
   "id": "236ff7916546c51f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:31:05.653139Z",
     "start_time": "2025-01-18T21:30:57.789580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    # Analyze specific terms (can include emojis and non-English terms)\n",
    "terms_of_interest = ['trump', 'biden', 'covid', 'vaccine', '😊', 'señor', '中国']\n",
    "analyze_specific_terms(all_fake_word_check, all_true_word_check, 'text', terms_of_interest)"
   ],
   "id": "5ea1cb7cd0163a8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Term 'trump':\n",
      "True news: 275 total occurrences, appears in 15,535 articles (44.65% of true news articles)\n",
      "Fake news: 671 total occurrences, appears in 13,764 articles (47.71% of fake news articles)\n",
      "\n",
      "Term 'biden':\n",
      "True news: 0 total occurrences, appears in 368 articles (1.06% of true news articles)\n",
      "Fake news: 3 total occurrences, appears in 311 articles (1.08% of fake news articles)\n",
      "\n",
      "Term 'covid':\n",
      "True news: 0 total occurrences, appears in 0 articles (0.00% of true news articles)\n",
      "Fake news: 1 total occurrences, appears in 2 articles (0.01% of fake news articles)\n",
      "\n",
      "Term 'vaccine':\n",
      "True news: 284 total occurrences, appears in 104 articles (0.30% of true news articles)\n",
      "Fake news: 977 total occurrences, appears in 122 articles (0.42% of fake news articles)\n",
      "\n",
      "Term '😊':\n",
      "True news: 0 total occurrences, appears in 0 articles (0.00% of true news articles)\n",
      "Fake news: 0 total occurrences, appears in 0 articles (0.00% of fake news articles)\n",
      "\n",
      "Term 'señor':\n",
      "True news: 2 total occurrences, appears in 5 articles (0.01% of true news articles)\n",
      "Fake news: 23 total occurrences, appears in 19 articles (0.07% of fake news articles)\n",
      "\n",
      "Term '中国':\n",
      "True news: 0 total occurrences, appears in 0 articles (0.00% of true news articles)\n",
      "Fake news: 0 total occurrences, appears in 0 articles (0.00% of fake news articles)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 5: Combine the datasets",
   "id": "d84e22060d7e0c77"
  },
  {
   "cell_type": "code",
   "id": "4bdeb2ea62e4fa39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:31:05.904518Z",
     "start_time": "2025-01-18T21:31:05.870590Z"
    }
   },
   "source": [
    "df = pd.concat([fake_df, true_df,WELF_dataset], axis=0).reset_index(drop=True)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:31:06.791308Z",
     "start_time": "2025-01-18T21:31:06.128801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#remove duplicates from the datasets\n",
    "df = df.drop_duplicates()"
   ],
   "id": "d0d552403e88184d",
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "54a04d438354b635",
   "metadata": {},
   "source": "## Step 6: Review the combined dataset"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:31:07.413080Z",
     "start_time": "2025-01-18T21:31:07.008144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "# Fill missing text with an empty string and convert to lowercase\n",
    "df['text'] = df['text'].fillna('')"
   ],
   "id": "beea60e013d7ed86",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Print out overview of the full dataset",
   "id": "cb29735f732e4f3c"
  },
  {
   "cell_type": "code",
   "id": "6dcbdf7a3d5ad687",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:31:07.603594Z",
     "start_time": "2025-01-18T21:31:07.596786Z"
    }
   },
   "source": [
    "print(\"Dataset Overview:\")\n",
    "print(df.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "                                               title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
      "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
      "\n",
      "                                                text  label  \n",
      "0  Donald Trump just couldn t wish all Americans ...      0  \n",
      "1  House Intelligence Committee Chairman Devin Nu...      0  \n",
      "2  On Friday, it was revealed that former Milwauk...      0  \n",
      "3  On Christmas day, Donald Trump announced that ...      0  \n",
      "4  Pope Francis used his annual Christmas Day mes...      0  \n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "6e67bd890fd57076",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:31:07.795091Z",
     "start_time": "2025-01-18T21:31:07.788516Z"
    }
   },
   "source": [
    "print(\"\\nClass Distribution:\")\n",
    "print(df['label'].value_counts())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution:\n",
      "label\n",
      "1    34791\n",
      "0    28848\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "c14e0113ab097afc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:31:07.981431Z",
     "start_time": "2025-01-18T21:31:07.970705Z"
    }
   },
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      "title    518\n",
      "text       0\n",
      "label      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "7344a731c01c486a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:31:08.197057Z",
     "start_time": "2025-01-18T21:31:08.191312Z"
    }
   },
   "source": [
    "print(df.tail(10))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    title  \\\n",
      "116961  Review: ‘Rogue One’ Leaves ‘Star Wars’ Fans Wa...   \n",
      "116963  Physician Aid in Dying Gains Acceptance in the...   \n",
      "116966             Letting Go Of Old Patterns Of Reaction   \n",
      "116967  Kris Kobach: Democrats Already Attacking Elect...   \n",
      "116968  Police Are Now Warning People To Take A Photo ...   \n",
      "116976  Tested by Russia, NATO Struggles to Stay Credi...   \n",
      "116982  Dallas, Roger Federer, Hillary Clinton: Your F...   \n",
      "116983  An Unlikely Contender Rises in France as the A...   \n",
      "116985  Determined to kill: Can tough gun laws end mas...   \n",
      "116990  Migrants Refuse To Leave Train At Refugee Camp...   \n",
      "\n",
      "                                                     text  label  \n",
      "116961  The great mystery of “Rogue One”  —   the big ...      1  \n",
      "116963  Judith Katherine Dunning had been waiting anxi...      1  \n",
      "116966  Leave a reply \\nMary O’Malley – A friend of mi...      0  \n",
      "116967    Kris Kobach of the Presidential Advisory Com...      1  \n",
      "116968  November 2015 Ads Police Are Now Warning Peopl...      0  \n",
      "116976  BRUSSELS  —   Six weeks before a critical summ...      1  \n",
      "116982  (Want to get this briefing by email? Here’s th...      1  \n",
      "116983  PARIS  —   In the age of Donald J. Trump, “Bre...      1  \n",
      "116985  The flag at Desert Hot Springs' Condor Gun Sho...      1  \n",
      "116990  Migrants Refuse To Leave Train At Refugee Camp...      1  \n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "6056b2ff122874b3",
   "metadata": {},
   "source": "### Optional: Remove Punctuation and Stopwords"
  },
  {
   "cell_type": "code",
   "id": "8d8d5755e50274d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:31:08.399614Z",
     "start_time": "2025-01-18T21:31:08.394815Z"
    }
   },
   "source": [
    "# Optional: Remove punctuation and stopwords (example with nltk)\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import string\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# df['text'] = df['text'].apply(lambda x: ' '.join(\n",
    "#     [word for word in word_tokenize(x) if word not in stop_words and word not in string.punctuation]\n",
    "# ))\n"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "d0217c2805227ab3",
   "metadata": {},
   "source": [
    "## Step 7: Split the Dataset\n",
    "train - 70%, validation - 15%, test - 15%\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e7b2a0ec4b36c34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:31:08.791550Z",
     "start_time": "2025-01-18T21:31:08.587858Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.15, random_state=42, stratify=df['label'])\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.1765, random_state=42, stratify=train_data['label'])"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "76fb33c6d0fc62a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:31:08.982736Z",
     "start_time": "2025-01-18T21:31:08.979421Z"
    }
   },
   "source": [
    "# Check the sizes\n",
    "print(f\"\\nTrain size: {len(train_data)}, Validation size: {len(val_data)}, Test size: {len(test_data)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train size: 44545, Validation size: 9548, Test size: 9546\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 8: Save the Splits",
   "id": "d0d8592faaeb25dd"
  },
  {
   "cell_type": "code",
   "id": "e8ba44d4af8ca0de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:31:12.059277Z",
     "start_time": "2025-01-18T21:31:09.182710Z"
    }
   },
   "source": [
    "# Save the splits\n",
    "train_data.to_csv(\"train.csv\", index=False)\n",
    "val_data.to_csv(\"val.csv\", index=False)\n",
    "test_data.to_csv(\"test.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
