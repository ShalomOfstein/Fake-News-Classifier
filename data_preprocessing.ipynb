{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9510b63ade3fef3c",
   "metadata": {},
   "source": [
    "data_preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7987ddc86023af08",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1cd8cdc7b8e135a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:04:32.331721Z",
     "start_time": "2025-01-17T11:04:32.327575Z"
    }
   },
   "source": [
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "bdc338b56ac9dcf4",
   "metadata": {},
   "source": [
    "# Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "ec790828d6644460",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:04:39.435494Z",
     "start_time": "2025-01-17T11:04:34.790792Z"
    }
   },
   "source": [
    "fake_path = \"data/original_data/fake.csv\"\n",
    "true_path = \"data/original_data/true.csv\"\n",
    "WELF_dataset_path = \"data/original_data/WELFake_Dataset.csv\"\n",
    "\n",
    "fake_df = pd.read_csv(fake_path)\n",
    "true_df = pd.read_csv(true_path)\n",
    "WELF_dataset = pd.read_csv(WELF_dataset_path)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "7e647984c7d24244",
   "metadata": {},
   "source": "# Add label column: Fake = 0, True = 1"
  },
  {
   "cell_type": "code",
   "id": "6c4bb2850d5d9d95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:04:42.416047Z",
     "start_time": "2025-01-17T11:04:42.409669Z"
    }
   },
   "source": [
    "fake_df['label'] = 0\n",
    "true_df['label'] = 1"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "a054786a1500b2f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Remove documents that dont have a text from the WELF dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "25e169b32eed9cf5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-17T11:04:47.856659Z",
     "start_time": "2025-01-17T11:04:47.833260Z"
    }
   },
   "source": [
    "WELF_dataset = WELF_dataset.dropna(subset=['text'])"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "rearrange the labels in WELF to be in sync with our dataset",
   "id": "6cea3385e48d1fee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:04:52.367687Z",
     "start_time": "2025-01-17T11:04:52.362463Z"
    }
   },
   "cell_type": "code",
   "source": "WELF_dataset['label'] = 1 - WELF_dataset['label']",
   "id": "3af93f96f1067c35",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "ad071e61e8c8254f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Leave only the title and text and label columns"
   ]
  },
  {
   "cell_type": "code",
   "id": "d7fbf08bd73bdf9d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-17T11:04:55.960198Z",
     "start_time": "2025-01-17T11:04:55.941400Z"
    }
   },
   "source": [
    "fake_df = fake_df[['title', 'text', 'label']]\n",
    "true_df = true_df[['title', 'text', 'label']]\n",
    "WELF_dataset = WELF_dataset[['title', 'text', 'label']]"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "check how many documents are in the WELF dataset",
   "id": "b9e81fafa8b6f5bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:04:58.272030Z",
     "start_time": "2025-01-17T11:04:58.266007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"size of WELF dataset: \", len(WELF_dataset))\n",
    "print(\"size of fake dataset: \", len(fake_df))\n",
    "print(\"size of true dataset: \", len(true_df))"
   ],
   "id": "83343b098dd6fdf3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of WELF dataset:  72095\n",
      "size of fake dataset:  23481\n",
      "size of true dataset:  21417\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "check how many fake and true documents are in the WELF dataset",
   "id": "ffa0889c1eccd35c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:04:59.996082Z",
     "start_time": "2025-01-17T11:04:59.979718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"number of fake in WELF: \", len(WELF_dataset[WELF_dataset['label'] == 0]))\n",
    "print(\"number of true in WELF: \", len(WELF_dataset[WELF_dataset['label'] == 1]))"
   ],
   "id": "c61dd59e3542ed62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of fake in WELF:  37067\n",
      "number of true in WELF:  35028\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Cleaning and analysis\n",
   "id": "2040b9d257c70338"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "check for each dataset how many times the frase (Reuters) appears in the text column for each label ",
   "id": "8c74fd9d499f6653"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:05:05.366123Z",
     "start_time": "2025-01-17T11:05:03.048081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"number of (Reuters) in true: \", true_df['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in fake: \", fake_df['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in WELF 'true' label: \", WELF_dataset[WELF_dataset['label'] == 1]['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in WELF 'fake' label: \", WELF_dataset[WELF_dataset['label'] == 0]['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())"
   ],
   "id": "9a3029581d974923",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of (Reuters) in true:  21246\n",
      "number of (Reuters) in fake:  0\n",
      "number of (Reuters) in WELF 'true' label:  21256\n",
      "number of (Reuters) in WELF 'fake' label:  0\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "remove (Reuters) from the text column in all the datasets",
   "id": "33fd630b25e3ef7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:05:13.847054Z",
     "start_time": "2025-01-17T11:05:07.900875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true_df['text'] = true_df['text'].str.replace(r'^.*?\\(Reuters\\)\\s*-\\s*', '', regex=True)\n",
    "fake_df['text'] = fake_df['text'].str.replace(r'^.*?\\(Reuters\\)\\s*-\\s*', '', regex=True)\n",
    "WELF_dataset['text'] = WELF_dataset['text'].str.replace(r'^.*?\\(Reuters\\)\\s*-\\s*', '', regex=True)\n",
    "\n",
    "#print out the number of (Reuters) in the text column for each label\n",
    "print(\"number of (Reuters) in true: \", true_df['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in fake: \", fake_df['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in WELF 'true' label: \", WELF_dataset[WELF_dataset['label'] == 1]['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in WELF 'fake' label: \", WELF_dataset[WELF_dataset['label'] == 0]['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())"
   ],
   "id": "6ce3753e7749b313",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of (Reuters) in true:  1\n",
      "number of (Reuters) in fake:  0\n",
      "number of (Reuters) in WELF 'true' label:  1\n",
      "number of (Reuters) in WELF 'fake' label:  0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "conclusion: there is atext with the frase (Reuters) twice!\n",
    "print the entry where the remaining (Reuters) is in the text column"
   ],
   "id": "ce2a768efc9aad90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:05:21.677738Z",
     "start_time": "2025-01-17T11:05:18.638229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(true_df[true_df['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*')])\n",
    "print(WELF_dataset[WELF_dataset['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*')])"
   ],
   "id": "dc01b6964578a54b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  title  \\\n",
      "5882  The Trump presidency on Jan 29 at 4:12 P.M. ES...   \n",
      "\n",
      "                                                   text  label  \n",
      "5882  Jan 29 (Reuters) - Highlights of the day for U...      1  \n",
      "                                                   title  \\\n",
      "26543  The Trump presidency on Jan 29 at 4:12 P.M. ES...   \n",
      "\n",
      "                                                    text  label  \n",
      "26543  Jan 29 (Reuters) - Highlights of the day for U...      1  \n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "remove the final (Reuters) from the text column in all the datasets",
   "id": "2b4251055b87920e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:05:48.552435Z",
     "start_time": "2025-01-17T11:05:39.333768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true_df['text'] = true_df['text'].str.replace(r'^.*?\\(Reuters\\)\\s*-\\s*', '', regex=True)\n",
    "fake_df['text'] = fake_df['text'].str.replace(r'^.*?\\(Reuters\\)\\s*-\\s*', '', regex=True)\n",
    "WELF_dataset['text'] = WELF_dataset['text'].str.replace(r'^.*?\\(Reuters\\)\\s*-\\s*', '', regex=True)\n",
    "\n",
    "#print out the number of (Reuters) in the text column for each label\n",
    "print(\"number of (Reuters) in true: \", true_df['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in fake: \", fake_df['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in WELF 'true' label: \", WELF_dataset[WELF_dataset['label'] == 1]['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())\n",
    "print(\"number of (Reuters) in WELF 'fake' label: \", WELF_dataset[WELF_dataset['label'] == 0]['text'].str.contains(r'^.*?\\(Reuters\\)\\s*-\\s*').sum())"
   ],
   "id": "15b7480fb39d5892",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of (Reuters) in true:  0\n",
      "number of (Reuters) in fake:  0\n",
      "number of (Reuters) in WELF 'true' label:  0\n",
      "number of (Reuters) in WELF 'fake' label:  0\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Look for more frases that are exclusive to one of the labels",
   "id": "74147a8b64eb2615"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:06:32.733521Z",
     "start_time": "2025-01-17T11:06:32.671591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_fake_word_check = pd.concat( [fake_df, WELF_dataset[WELF_dataset['label'] == 0]], axis=0).reset_index(drop=True)\n",
    "all_true_word_check = pd.concat( [true_df, WELF_dataset[WELF_dataset['label'] == 1]], axis=0).reset_index(drop=True)"
   ],
   "id": "8d6cbb67f258b4fa",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:06:36.744491Z",
     "start_time": "2025-01-17T11:06:36.734477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"length of all_fake_word_check: \", len(all_fake_word_check))\n",
    "print(\"length of all_true_word_check: \", len(all_true_word_check))"
   ],
   "id": "b8fc086c3c642f45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of all_fake_word_check:  60548\n",
      "length of all_true_word_check:  56445\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:06:40.033335Z",
     "start_time": "2025-01-17T11:06:38.035919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#remove duplicates from the datasets\n",
    "all_fake_word_check = all_fake_word_check.drop_duplicates()\n",
    "all_true_word_check = all_true_word_check.drop_duplicates()\n"
   ],
   "id": "8530c809fdfc90b0",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:06:43.762330Z",
     "start_time": "2025-01-17T11:06:43.753861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"length of all_fake_word_check: \", len(all_fake_word_check))\n",
    "print(\"length of all_true_word_check: \", len(all_true_word_check))"
   ],
   "id": "3fb435fa1d63131f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of all_fake_word_check:  28848\n",
      "length of all_true_word_check:  34791\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:08:07.514129Z",
     "start_time": "2025-01-17T11:06:56.166622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_text(text: str, keep_symbols: bool = True) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocess text while preserving multilingual characters and optional symbols.\n",
    "    \n",
    "    Parameters:\n",
    "        text: Input text\n",
    "        keep_symbols: If True, keeps emoji and special symbols\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    if keep_symbols:\n",
    "        # Keep unicode characters (including emojis) and letters, remove unwanted characters\n",
    "        # This pattern keeps all Unicode letters, numbers, emojis, and common symbols\n",
    "        text = re.sub(r'[^\\w\\s\\u0080-\\u10FFFF\\']+', ' ', text)\n",
    "    else:\n",
    "        # Remove special characters but keep apostrophes for contractions\n",
    "        text = re.sub(r'[^a-zA-Z\\']+', ' ', text)\n",
    "\n",
    "    # Remove single quotes if they're not part of contractions\n",
    "    text = re.sub(r'\\s\\'|\\'\\s|^\\'|\\'$', ' ', text)\n",
    "\n",
    "    # Split into words and remove empty strings\n",
    "    words = [word.strip() for word in text.split()]\n",
    "\n",
    "    # Remove single-character words and empty strings\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "\n",
    "    return words\n",
    "\n",
    "def analyze_word_frequencies(fake_df: pd.DataFrame, true_df: pd.DataFrame,\n",
    "                             text_column: str, min_frequency: int = 2,\n",
    "                             keep_symbols: bool = True) -> Tuple[Dict, Dict, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Analyze word frequencies in fake and true news articles.\n",
    "    Returns word frequencies and article counts for both categories.\n",
    "    \"\"\"\n",
    "    # Initialize Counters for word frequencies and article appearances\n",
    "    fake_vocab = Counter()\n",
    "    true_vocab = Counter()\n",
    "    fake_article_counts = Counter()\n",
    "    true_article_counts = Counter()\n",
    "\n",
    "    # Process fake news\n",
    "    for _, row in fake_df.iterrows():\n",
    "        text = row[text_column]\n",
    "        words = preprocess_text(text, keep_symbols)\n",
    "        # Update total word frequency\n",
    "        fake_vocab.update(words)\n",
    "        # Update article counts (count each word only once per article)\n",
    "        fake_article_counts.update(set(words))\n",
    "\n",
    "    # Process true news\n",
    "    for _, row in true_df.iterrows():\n",
    "        text = row[text_column]\n",
    "        words = preprocess_text(text, keep_symbols)\n",
    "        # Update total word frequency\n",
    "        true_vocab.update(words)\n",
    "        # Update article counts (count each word only once per article)\n",
    "        true_article_counts.update(set(words))\n",
    "\n",
    "    # Find exclusive words (filtering by minimum frequency)\n",
    "    fake_exclusive = {}\n",
    "    true_exclusive = {}\n",
    "\n",
    "    for word, count in fake_vocab.items():\n",
    "        if word not in true_vocab and count >= min_frequency:\n",
    "            fake_exclusive[word] = {\n",
    "                'total_frequency': count,\n",
    "                'article_count': fake_article_counts[word],\n",
    "                'article_percentage': (fake_article_counts[word] / len(fake_df)) * 100\n",
    "            }\n",
    "\n",
    "    for word, count in true_vocab.items():\n",
    "        if word not in fake_vocab and count >= min_frequency:\n",
    "            true_exclusive[word] = {\n",
    "                'total_frequency': count,\n",
    "                'article_count': true_article_counts[word],\n",
    "                'article_percentage': (true_article_counts[word] / len(true_df)) * 100\n",
    "            }\n",
    "\n",
    "    # Sort by total frequency\n",
    "    fake_exclusive_sorted = dict(sorted(fake_exclusive.items(),\n",
    "                                        key=lambda x: x[1]['total_frequency'],\n",
    "                                        reverse=True))\n",
    "    true_exclusive_sorted = dict(sorted(true_exclusive.items(),\n",
    "                                        key=lambda x: x[1]['total_frequency'],\n",
    "                                        reverse=True))\n",
    "\n",
    "    return fake_vocab, true_vocab, fake_exclusive_sorted, true_exclusive_sorted\n",
    "\n",
    "def print_analysis_results(fake_vocab: Dict, true_vocab: Dict,\n",
    "                           fake_exclusive: Dict, true_exclusive: Dict,\n",
    "                           n_words: int = 10):\n",
    "    \"\"\"\n",
    "    Print detailed analysis results including article counts.\n",
    "    \"\"\"\n",
    "    print(\"\\nVocabulary Statistics:\")\n",
    "    print(f\"Total unique words in fake news vocabulary: {len(fake_vocab):,}\")\n",
    "    print(f\"Total unique words in true news vocabulary: {len(true_vocab):,}\")\n",
    "    print(f\"Exclusive words in fake news: {len(fake_exclusive):,}\")\n",
    "    print(f\"Exclusive words in true news: {len(true_exclusive):,}\")\n",
    "\n",
    "    print(\"\\nTop exclusive words in fake news:\")\n",
    "    for word, stats in list(fake_exclusive.items())[:n_words]:\n",
    "        print(f\"'{word}': {stats['total_frequency']:,} total occurrences, \"\n",
    "              f\"appears in {stats['article_count']:,} articles \"\n",
    "              f\"({stats['article_percentage']:.1f}% of fake news articles)\")\n",
    "\n",
    "    print(\"\\nTop exclusive words in true news:\")\n",
    "    for word, stats in list(true_exclusive.items())[:n_words]:\n",
    "        print(f\"'{word}': {stats['total_frequency']:,} total occurrences, \"\n",
    "              f\"appears in {stats['article_count']:,} articles \"\n",
    "              f\"({stats['article_percentage']:.1f}% of true news articles)\")\n",
    "\n",
    "def analyze_specific_terms(fake_df: pd.DataFrame, true_df: pd.DataFrame,\n",
    "                           text_column: str, terms: List[str]):\n",
    "    \"\"\"\n",
    "    Analyze frequency of specific terms in both datasets.\n",
    "    \"\"\"\n",
    "    for term in terms:\n",
    "        # Count occurrences\n",
    "        true_count = true_df[text_column].str.count(term).sum()\n",
    "        fake_count = fake_df[text_column].str.count(term).sum()\n",
    "\n",
    "        # Count articles containing the term\n",
    "        true_articles = true_df[text_column].str.contains(term, case=False).sum()\n",
    "        fake_articles = fake_df[text_column].str.contains(term, case=False).sum()\n",
    "\n",
    "        true_percentage = (true_articles / len(true_df)) * 100\n",
    "        fake_percentage = (fake_articles / len(fake_df)) * 100\n",
    "\n",
    "        print(f\"\\nTerm '{term}':\")\n",
    "        print(f\"True news: {true_count:,} total occurrences, \"\n",
    "              f\"appears in {true_articles:,} articles \"\n",
    "              f\"({true_percentage:.2f}% of true news articles)\")\n",
    "        print(f\"Fake news: {fake_count:,} total occurrences, \"\n",
    "              f\"appears in {fake_articles:,} articles \"\n",
    "              f\"({fake_percentage:.2f}% of fake news articles)\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming fake_df and true_df are your separated DataFrames\n",
    "    fake_vocab, true_vocab, fake_exclusive, true_exclusive = analyze_word_frequencies(\n",
    "        all_fake_word_check, all_true_word_check,\n",
    "        text_column='text',\n",
    "        min_frequency=2,\n",
    "        keep_symbols=True  # Set to True to keep emojis and special characters\n",
    "    )\n",
    "\n",
    "    # Print general analysis\n",
    "    print_analysis_results(fake_vocab, true_vocab, fake_exclusive, true_exclusive)\n",
    "\n",
    "    # Analyze specific terms (can include emojis and non-English terms)\n",
    "    terms_of_interest = ['trump', 'biden', 'covid', 'vaccine', '😊', 'señor', '中国']\n",
    "    analyze_specific_terms(all_fake_word_check, all_true_word_check, 'text', terms_of_interest)"
   ],
   "id": "75807659bf095d3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary Statistics:\n",
      "Total unique words in fake news vocabulary: 185,021\n",
      "Total unique words in true news vocabulary: 127,253\n",
      "Exclusive words in fake news: 38,083\n",
      "Exclusive words in true news: 24,928\n",
      "\n",
      "Top exclusive words in fake news:\n",
      "'что': 1,423 total occurrences, appears in 148 articles (0.5% of fake news articles)\n",
      "'не': 1,170 total occurrences, appears in 137 articles (0.5% of fake news articles)\n",
      "'21wire': 1,117 total occurrences, appears in 552 articles (1.9% of fake news articles)\n",
      "'quot': 1,012 total occurrences, appears in 13 articles (0.0% of fake news articles)\n",
      "'по': 766 total occurrences, appears in 149 articles (0.5% of fake news articles)\n",
      "'это': 727 total occurrences, appears in 101 articles (0.4% of fake news articles)\n",
      "'как': 603 total occurrences, appears in 133 articles (0.5% of fake news articles)\n",
      "'то': 482 total occurrences, appears in 105 articles (0.4% of fake news articles)\n",
      "'somodevilla': 481 total occurrences, appears in 480 articles (1.7% of fake news articles)\n",
      "'2017the': 463 total occurrences, appears in 395 articles (1.4% of fake news articles)\n",
      "\n",
      "Top exclusive words in true news:\n",
      "'rakhine': 908 total occurrences, appears in 273 articles (0.8% of true news articles)\n",
      "'mnangagwa': 395 total occurrences, appears in 92 articles (0.3% of true news articles)\n",
      "'tmsnrt': 334 total occurrences, appears in 265 articles (0.8% of true news articles)\n",
      "'barnier': 283 total occurrences, appears in 116 articles (0.3% of true news articles)\n",
      "'marawi': 273 total occurrences, appears in 71 articles (0.2% of true news articles)\n",
      "'babis': 267 total occurrences, appears in 41 articles (0.1% of true news articles)\n",
      "'durst': 242 total occurrences, appears in 11 articles (0.0% of true news articles)\n",
      "'pamkeynen': 238 total occurrences, appears in 238 articles (0.7% of true news articles)\n",
      "'kem': 219 total occurrences, appears in 59 articles (0.2% of true news articles)\n",
      "'sokha': 217 total occurrences, appears in 59 articles (0.2% of true news articles)\n",
      "\n",
      "Term 'trump':\n",
      "True news: 275 total occurrences, appears in 15,535 articles (44.65% of true news articles)\n",
      "Fake news: 671 total occurrences, appears in 13,764 articles (47.71% of fake news articles)\n",
      "\n",
      "Term 'biden':\n",
      "True news: 0 total occurrences, appears in 368 articles (1.06% of true news articles)\n",
      "Fake news: 3 total occurrences, appears in 311 articles (1.08% of fake news articles)\n",
      "\n",
      "Term 'covid':\n",
      "True news: 0 total occurrences, appears in 0 articles (0.00% of true news articles)\n",
      "Fake news: 1 total occurrences, appears in 2 articles (0.01% of fake news articles)\n",
      "\n",
      "Term 'vaccine':\n",
      "True news: 284 total occurrences, appears in 104 articles (0.30% of true news articles)\n",
      "Fake news: 977 total occurrences, appears in 122 articles (0.42% of fake news articles)\n",
      "\n",
      "Term '😊':\n",
      "True news: 0 total occurrences, appears in 0 articles (0.00% of true news articles)\n",
      "Fake news: 0 total occurrences, appears in 0 articles (0.00% of fake news articles)\n",
      "\n",
      "Term 'señor':\n",
      "True news: 2 total occurrences, appears in 5 articles (0.01% of true news articles)\n",
      "Fake news: 23 total occurrences, appears in 19 articles (0.07% of fake news articles)\n",
      "\n",
      "Term '中国':\n",
      "True news: 0 total occurrences, appears in 0 articles (0.00% of true news articles)\n",
      "Fake news: 0 total occurrences, appears in 0 articles (0.00% of fake news articles)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# next part",
   "id": "d84e22060d7e0c77"
  },
  {
   "cell_type": "markdown",
   "id": "774cbdbb0e6ec32a",
   "metadata": {},
   "source": [
    "# Combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "4bdeb2ea62e4fa39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:09:07.097127Z",
     "start_time": "2025-01-17T11:09:07.034510Z"
    }
   },
   "source": [
    "df = pd.concat([fake_df, true_df,WELF_dataset], axis=0).reset_index(drop=True)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:09:09.888190Z",
     "start_time": "2025-01-17T11:09:08.381541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#remove duplicates from the datasets\n",
    "df = df.drop_duplicates()"
   ],
   "id": "d0d552403e88184d",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "54a04d438354b635",
   "metadata": {},
   "source": [
    "# Step 1: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "id": "6dcbdf7a3d5ad687",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:09:15.585051Z",
     "start_time": "2025-01-17T11:09:15.565443Z"
    }
   },
   "source": [
    "print(\"Dataset Overview:\")\n",
    "print(df.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "                                               title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
      "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
      "\n",
      "                                                text  label  \n",
      "0  Donald Trump just couldn t wish all Americans ...      0  \n",
      "1  House Intelligence Committee Chairman Devin Nu...      0  \n",
      "2  On Friday, it was revealed that former Milwauk...      0  \n",
      "3  On Christmas day, Donald Trump announced that ...      0  \n",
      "4  Pope Francis used his annual Christmas Day mes...      0  \n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "6e67bd890fd57076",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:09:21.697965Z",
     "start_time": "2025-01-17T11:09:21.682704Z"
    }
   },
   "source": [
    "print(\"\\nClass Distribution:\")\n",
    "print(df['label'].value_counts())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution:\n",
      "label\n",
      "1    34791\n",
      "0    28848\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "c14e0113ab097afc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:09:22.859902Z",
     "start_time": "2025-01-17T11:09:22.837060Z"
    }
   },
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      "title    518\n",
      "text       0\n",
      "label      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "7344a731c01c486a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:09:24.407736Z",
     "start_time": "2025-01-17T11:09:24.400222Z"
    }
   },
   "source": [
    "print(df.tail(10))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    title  \\\n",
      "116961  Review: ‘Rogue One’ Leaves ‘Star Wars’ Fans Wa...   \n",
      "116963  Physician Aid in Dying Gains Acceptance in the...   \n",
      "116966             Letting Go Of Old Patterns Of Reaction   \n",
      "116967  Kris Kobach: Democrats Already Attacking Elect...   \n",
      "116968  Police Are Now Warning People To Take A Photo ...   \n",
      "116976  Tested by Russia, NATO Struggles to Stay Credi...   \n",
      "116982  Dallas, Roger Federer, Hillary Clinton: Your F...   \n",
      "116983  An Unlikely Contender Rises in France as the A...   \n",
      "116985  Determined to kill: Can tough gun laws end mas...   \n",
      "116990  Migrants Refuse To Leave Train At Refugee Camp...   \n",
      "\n",
      "                                                     text  label  \n",
      "116961  The great mystery of “Rogue One”  —   the big ...      1  \n",
      "116963  Judith Katherine Dunning had been waiting anxi...      1  \n",
      "116966  Leave a reply \\nMary O’Malley – A friend of mi...      0  \n",
      "116967    Kris Kobach of the Presidential Advisory Com...      1  \n",
      "116968  November 2015 Ads Police Are Now Warning Peopl...      0  \n",
      "116976  BRUSSELS  —   Six weeks before a critical summ...      1  \n",
      "116982  (Want to get this briefing by email? Here’s th...      1  \n",
      "116983  PARIS  —   In the age of Donald J. Trump, “Bre...      1  \n",
      "116985  The flag at Desert Hot Springs' Condor Gun Sho...      1  \n",
      "116990  Migrants Refuse To Leave Train At Refugee Camp...      1  \n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "6056b2ff122874b3",
   "metadata": {},
   "source": [
    "# Step 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a11ea654bcbaa5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:09:34.990476Z",
     "start_time": "2025-01-17T11:09:34.175650Z"
    }
   },
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "# Fill missing text with an empty string and convert to lowercase\n",
    "df['text'] = df['text'].fillna('')"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "8d8d5755e50274d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:09:36.888320Z",
     "start_time": "2025-01-17T11:09:36.882096Z"
    }
   },
   "source": [
    "# Optional: Remove punctuation and stopwords (example with nltk)\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import string\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# df['text'] = df['text'].apply(lambda x: ' '.join(\n",
    "#     [word for word in word_tokenize(x) if word not in stop_words and word not in string.punctuation]\n",
    "# ))\n"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "d0217c2805227ab3",
   "metadata": {},
   "source": [
    "# Step 3: Train-Test Split\n",
    "train - 70%, validation - 15%, test - 15%\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e7b2a0ec4b36c34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:09:40.492363Z",
     "start_time": "2025-01-17T11:09:40.197917Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.15, random_state=42, stratify=df['label'])\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.1765, random_state=42, stratify=train_data['label'])"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "76fb33c6d0fc62a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:09:49.857610Z",
     "start_time": "2025-01-17T11:09:49.849498Z"
    }
   },
   "source": [
    "# Check the sizes\n",
    "print(f\"\\nTrain size: {len(train_data)}, Validation size: {len(val_data)}, Test size: {len(test_data)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train size: 44545, Validation size: 9548, Test size: 9546\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "e8ba44d4af8ca0de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:10:00.382066Z",
     "start_time": "2025-01-17T11:09:51.897252Z"
    }
   },
   "source": [
    "# Save the splits\n",
    "train_data.to_csv(\"train.csv\", index=False)\n",
    "val_data.to_csv(\"val.csv\", index=False)\n",
    "test_data.to_csv(\"test.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6755eafae399c10b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
